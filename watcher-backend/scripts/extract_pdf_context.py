"""
Extractor de Contexto desde PDFs Presupuestarios
Procesa Ley de Presupuesto y Mensaje de Elevaci√≥n para extraer keywords y prioridades

Autor: Watcher Fiscal Agent
"""

import pdfplumber
import json
import re
from pathlib import Path
from collections import Counter
from typing import Dict, List

# Rutas
BASE_DIR = Path(__file__).parent.parent.parent.parent
PDF_DIR = BASE_DIR / "watcher-doc" / "data"
OUTPUT_DIR = BASE_DIR / "watcher-doc"

# PDFs a procesar
LEY_PRESUPUESTO = PDF_DIR / "Ley-de-Presupuesto-L-11014.pdf"
MENSAJE_ELEVACION = PDF_DIR / "Mensaje-de-Elevacion_Presupuesto-2025.pdf"

# Keywords ignoradas (stopwords en espa√±ol)
STOPWORDS = set([
    'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'ser', 'se', 'no', 'haber',
    'por', 'con', 'su', 'para', 'como', 'estar', 'tener', 'le', 'lo', 'todo',
    'pero', 'm√°s', 'hacer', 'o', 'poder', 'decir', 'este', 'ir', 'otro', 'ese',
    'si', 'me', 'ya', 'ver', 'porque', 'dar', 'cuando', '√©l', 'muy', 'sin', 'vez',
    'mucho', 'saber', 'qu√©', 'sobre', 'mi', 'alguno', 'mismo', 'yo', 'tambi√©n',
    'hasta', 'a√±o', 'dos', 'querer', 'entre', 'as√≠', 'primero', 'desde', 'grande',
    'eso', 'ni', 'nos', 'llegar', 'pasar', 'tiempo', 'ella', 's√≠', 'd√≠a', 'uno',
    'bien', 'poco', 'deber', 'entonces', 'poner', 'cosa', 'tanto', 'hombre', 'parecer',
    'nuestro', 'tan', 'donde', 'ahora', 'parte', 'despu√©s', 'vida', 'quedar', 'siempre',
    'creer', 'hablar', 'llevar', 'dejar', 'nada', 'cada', 'seguir', 'menos', 'nuevo',
    'encontrar', 'algo', 'solo', 'decir', 'casa', 'aunque', 'pues', 'ante', 'bajo',
    'art√≠culo', 'art√≠culos', 'articulo', 'articulos', 'ley', 'leyes', 'los', 'las',
    'del', 'al', 'ante', 'mediante', 'seg√∫n', 'durante'
])


def extract_text_from_pdf(pdf_path: Path, max_pages: int = None) -> str:
    """Extrae texto completo de un PDF"""
    print(f"\nüìñ Extrayendo texto de: {pdf_path.name}")
    
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        total_pages = len(pdf.pages)
        pages_to_process = min(max_pages, total_pages) if max_pages else total_pages
        
        print(f"   Total p√°ginas: {total_pages}, procesando: {pages_to_process}")
        
        for i, page in enumerate(pdf.pages[:pages_to_process], 1):
            try:
                page_text = page.extract_text() or ""
                text += page_text + "\n"
                if i % 10 == 0:
                    print(f"   ‚úì P√°ginas procesadas: {i}/{pages_to_process}", end='\r')
            except Exception as e:
                print(f"\n   ‚ö† Error en p√°gina {i}: {e}")
                continue
        
        print(f"\n   ‚úì Extra√≠dos {len(text)} caracteres")
    
    return text


def extract_keywords(text: str, min_freq: int = 3, min_length: int = 5) -> List[tuple]:
    """Extrae keywords por frecuencia"""
    # Limpiar y normalizar texto
    text = text.upper()
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Extraer palabras
    words = text.split()
    
    # Filtrar stopwords y palabras cortas
    words = [
        word for word in words 
        if len(word) >= min_length and word.lower() not in STOPWORDS
    ]
    
    # Contar frecuencias
    counter = Counter(words)
    
    # Retornar top keywords
    return counter.most_common(100)


def extract_priority_topics(text: str) -> Dict[str, List[str]]:
    """Extrae t√≥picos prioritarios basados en keywords clave"""
    text_upper = text.upper()
    
    # Diccionario de temas con sus keywords asociadas
    topics = {
        'salud': ['HOSPITAL', 'M√âDICO', 'MEDICINA', 'VACUNA', 'TRATAMIENTO', 
                  'PACIENTE', 'SANITARIO', 'CL√çNICA', 'SALUD'],
        'educacion': ['ESCUELA', 'DOCENTE', 'ALUMNO', 'EDUCATIVO', 'ENSE√ëANZA',
                      'MAESTRO', 'UNIVERSIDAD', 'ESTUDIANTE', 'EDUCACI√ìN'],
        'infraestructura': ['OBRA', 'CONSTRUCCI√ìN', 'VIAL', 'RUTA', 'CAMINO',
                            'PAVIMENTO', 'INFRAESTRUCTURA', 'EDIFICIO'],
        'seguridad': ['POLIC√çA', 'SEGURIDAD', 'PREVENCI√ìN', 'EMERGENCIA',
                     'BOMBERO', 'PROTECCI√ìN', 'VIGILANCIA'],
        'desarrollo_social': ['SOCIAL', 'COMUNIDAD', 'ASISTENCIA', 'SUBSIDIO',
                             'FAMILIA', 'POBREZA', 'INCLUSI√ìN'],
        'economia': ['ECON√ìMICO', 'FINANCIERO', 'PRESUPUESTO', 'FISCAL',
                    'TRIBUTARIO', 'IMPUESTO', 'RECAUDACI√ìN'],
        'ambiente': ['AMBIENTE', 'AMBIENTAL', 'ECOLOG√çA', 'RESIDUO',
                    'AGUA', 'ENERG√çA', 'SOSTENIBLE', 'SUSTENTABLE'],
        'produccion': ['PRODUCCI√ìN', 'INDUSTRIA', 'AGR√çCOLA', 'GANADERO',
                      'COMERCIO', 'EXPORTACI√ìN', 'TECNOLOG√çA']
    }
    
    # Buscar menciones de cada tema
    topic_keywords = {}
    for topic, keywords in topics.items():
        found_keywords = []
        for keyword in keywords:
            count = text_upper.count(keyword)
            if count > 0:
                found_keywords.append(f"{keyword.lower()} ({count})")
        
        if found_keywords:
            topic_keywords[topic] = found_keywords
    
    return topic_keywords


def generate_semantic_vocabulary(keywords_ley: List[tuple], keywords_mensaje: List[tuple], topics: Dict) -> Dict:
    """Genera vocabulario sem√°ntico fiscal"""
    print("\nüìù Generando vocabulario sem√°ntico...")
    
    # Combinar keywords de ambos documentos
    all_keywords = set()
    for word, _ in keywords_ley[:50]:
        all_keywords.add(word.lower())
    for word, _ in keywords_mensaje[:50]:
        all_keywords.add(word.lower())
    
    # Vocabulario base con sin√≥nimos
    vocab = {
        'licitacion': ['contrataci√≥n', 'adjudicaci√≥n', 'concurso', 'llamado', 'puja'],
        'decreto': ['resoluci√≥n', 'disposici√≥n', 'acto administrativo', 'norma'],
        'subsidio': ['asistencia', 'ayuda econ√≥mica', 'transferencia', 'aporte'],
        'obra': ['construcci√≥n', 'infraestructura', 'proyecto', 'edificaci√≥n'],
        'programa': ['plan', 'proyecto', 'iniciativa', 'pol√≠tica p√∫blica'],
        'presupuesto': ['cr√©dito', 'partida', 'asignaci√≥n', 'recursos'],
        'ministerio': ['jurisdicci√≥n', 'cartera', 'organismo', 'secretar√≠a'],
        'empleado': ['agente', 'funcionario', 'personal', 'trabajador'],
        'gasto': ['erogaci√≥n', 'egreso', 'desembolso', 'inversi√≥n'],
        'ingreso': ['recurso', 'recaudaci√≥n', 'renta', 'tributo']
    }
    
    # Agregar keywords por tema
    for topic, keywords in topics.items():
        # Extraer solo las palabras (sin counts)
        words = [kw.split(' (')[0] for kw in keywords]
        vocab[topic] = words[:10]  # Top 10 por tema
    
    # Agregar keywords generales
    vocab['keywords_generales'] = list(all_keywords)[:100]
    
    print(f"   ‚úì Vocabulario generado con {len(vocab)} categor√≠as")
    return vocab


def extract_priorities_summary(text: str, max_length: int = 2000) -> str:
    """Extrae resumen de prioridades del Mensaje de Elevaci√≥n"""
    # Buscar secci√≥n de prioridades (t√≠picamente en primeras p√°ginas)
    text_lines = text.split('\n')
    
    # Buscar palabras clave de secciones importantes
    priority_keywords = ['PRIORITARIO', 'OBJETIVO', 'META', 'PRIORIDAD', 'ESTRAT√âG']
    
    priority_lines = []
    for i, line in enumerate(text_lines[:200]):  # Primeras 200 l√≠neas
        line_upper = line.upper()
        if any(kw in line_upper for kw in priority_keywords):
            # Agregar contexto (3 l√≠neas antes y 5 despu√©s)
            start = max(0, i - 3)
            end = min(len(text_lines), i + 6)
            priority_lines.extend(text_lines[start:end])
    
    summary = '\n'.join(priority_lines)[:max_length]
    return summary if summary else text[:max_length]


def main():
    """Funci√≥n principal"""
    print(f"\n{'#'*80}")
    print("# EXTRACTOR DE CONTEXTO PRESUPUESTARIO")
    print(f"{'#'*80}")
    
    # 1. Extraer texto de Ley de Presupuesto
    print(f"\n{'='*80}")
    print("FASE 1: LEY DE PRESUPUESTO")
    print(f"{'='*80}")
    
    if not LEY_PRESUPUESTO.exists():
        print(f"‚úó No encontrado: {LEY_PRESUPUESTO}")
        return
    
    texto_ley = extract_text_from_pdf(LEY_PRESUPUESTO)
    keywords_ley = extract_keywords(texto_ley)
    
    print("\nüìä Top 10 Keywords Ley de Presupuesto:")
    for word, count in keywords_ley[:10]:
        print(f"   ‚Ä¢ {word:<30} {count:>4} menciones")
    
    # 2. Extraer texto de Mensaje de Elevaci√≥n
    print(f"\n{'='*80}")
    print("FASE 2: MENSAJE DE ELEVACI√ìN")
    print(f"{'='*80}")
    
    if not MENSAJE_ELEVACION.exists():
        print(f"‚úó No encontrado: {MENSAJE_ELEVACION}")
        return
    
    texto_mensaje = extract_text_from_pdf(MENSAJE_ELEVACION, max_pages=30)
    keywords_mensaje = extract_keywords(texto_mensaje)
    
    print("\nüìä Top 10 Keywords Mensaje de Elevaci√≥n:")
    for word, count in keywords_mensaje[:10]:
        print(f"   ‚Ä¢ {word:<30} {count:>4} menciones")
    
    # 3. Extraer t√≥picos prioritarios
    print(f"\n{'='*80}")
    print("FASE 3: AN√ÅLISIS DE PRIORIDADES")
    print(f"{'='*80}")
    
    topics = extract_priority_topics(texto_mensaje)
    
    print("\nüìç T√≥picos Identificados:")
    for topic, keywords in sorted(topics.items()):
        print(f"   ‚Ä¢ {topic}: {len(keywords)} keywords")
        print(f"     {', '.join(keywords[:5])}")
    
    # 4. Generar vocabulario sem√°ntico
    vocab = generate_semantic_vocabulary(keywords_ley, keywords_mensaje, topics)
    
    # 5. Extraer resumen de prioridades
    priorities_summary = extract_priorities_summary(texto_mensaje)
    
    # 6. Guardar outputs
    print(f"\n{'='*80}")
    print("GUARDANDO ARCHIVOS")
    print(f"{'='*80}")
    
    # Vocabulario sem√°ntico
    vocab_path = OUTPUT_DIR / "vocabulario_semantico_fiscal.json"
    with open(vocab_path, 'w', encoding='utf-8') as f:
        json.dump(vocab, f, ensure_ascii=False, indent=2)
    print(f"‚úì Guardado: {vocab_path}")
    
    # Metas presupuestarias
    metas_path = OUTPUT_DIR / "metas_presupuestarias_2025.json"
    metas = {
        'keywords_ley': [{'word': w, 'count': c} for w, c in keywords_ley[:50]],
        'keywords_mensaje': [{'word': w, 'count': c} for w, c in keywords_mensaje[:50]],
        'topics': topics
    }
    with open(metas_path, 'w', encoding='utf-8') as f:
        json.dump(metas, f, ensure_ascii=False, indent=2)
    print(f"‚úì Guardado: {metas_path}")
    
    # Prioridades gubernamentales
    priorities_path = OUTPUT_DIR / "prioridades_gubernamentales.txt"
    with open(priorities_path, 'w', encoding='utf-8') as f:
        f.write(priorities_summary)
    print(f"‚úì Guardado: {priorities_path}")
    
    print(f"\n{'#'*80}")
    print("# ‚úÖ EXTRACCI√ìN COMPLETADA")
    print(f"{'#'*80}\n")


if __name__ == "__main__":
    main()



