{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# √âpica 0: Migraci√≥n OpenAI ‚Üí Google Gemini\n",
        "\n",
        "## Objetivo\n",
        "Validar la migraci√≥n completa del sistema Watcher desde OpenAI a Google Gemini como proveedor principal de LLM y embeddings.\n",
        "\n",
        "## Tickets cubiertos\n",
        "| Ticket | Descripci√≥n | Estado |\n",
        "|--------|-------------|--------|\n",
        "| 0.1 | Revocar API key OpenAI expuesta | ‚¨ú |\n",
        "| 0.2 | Instalar Google AI SDK | ‚¨ú |\n",
        "| 0.3 | Migrar EmbeddingService a Google | ‚¨ú |\n",
        "| 0.4 | Migrar DocumentProcessor a embeddings Google | ‚¨ú |\n",
        "| 0.5 | Migrar WatcherService a gemini-2.0-flash | ‚¨ú |\n",
        "| 0.6 | Migrar InsightReportingAgent a Gemini | ‚¨ú |\n",
        "| 0.7 | Actualizar AgentSystemConfig para nuevas keys | ‚¨ú |\n",
        "| 0.8 | Migrar LangChain a langchain-google-genai | ‚¨ú |\n",
        "| 0.9 | Re-indexar ChromaDB con nuevos embeddings | ‚¨ú |\n",
        "| 0.10 | Agregar Anthropic como provider opcional | ‚¨ú |\n",
        "\n",
        "## Componentes principales afectados\n",
        "- `embedding_service.py` ‚Äî Embeddings vectoriales (Google gemini-embedding-001)\n",
        "- `watcher_service.py` ‚Äî An√°lisis de fragmentos (Gemini 2.0 Flash)\n",
        "- `llm_provider.py` ‚Äî Abstracci√≥n multi-proveedor LLM\n",
        "- `config.py` ‚Äî Configuraci√≥n central\n",
        "- `agent_config.py` ‚Äî Configuraci√≥n del sistema de agentes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup del entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÖ Fecha de ejecuci√≥n: 2026-02-10 13:06:50\n",
            "üìÇ Backend dir: /Users/germanevangelisti/watcher-agent/watcher-monolith/backend\n",
            "üêç Python: 3.9.10 (main, Oct 11 2024, 16:02:49) \n",
            "[Clang 15.0.0 (clang-1500.3.9.4)]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import importlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Agregar el backend al path\n",
        "BACKEND_DIR = Path(\"../watcher-monolith/backend\").resolve()\n",
        "if str(BACKEND_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(BACKEND_DIR))\n",
        "\n",
        "# Cargar variables de entorno desde el .env del backend\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(BACKEND_DIR / \".env\", override=True)\n",
        "\n",
        "# Forzar reload de m√≥dulos del backend para tomar cambios recientes\n",
        "# (√∫til cuando se editan archivos entre ejecuciones sin reiniciar el kernel)\n",
        "for mod_name in list(sys.modules.keys()):\n",
        "    if mod_name.startswith(\"app.\") or mod_name.startswith(\"agents.\"):\n",
        "        del sys.modules[mod_name]\n",
        "\n",
        "# Resultado tracker para resumen final\n",
        "RESULTS = {}\n",
        "\n",
        "def log_result(ticket: str, name: str, passed: bool, details: str = \"\", skipped: bool = False):\n",
        "    \"\"\"Registra el resultado de un test.\"\"\"\n",
        "    if skipped:\n",
        "        status = \"‚è≠Ô∏è SKIP\"\n",
        "    else:\n",
        "        status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
        "    RESULTS[ticket] = {\"name\": name, \"passed\": passed, \"details\": details, \"skipped\": skipped}\n",
        "    print(f\"{status} | {ticket}: {name}\")\n",
        "    if details:\n",
        "        print(f\"       ‚Üí {details}\")\n",
        "\n",
        "print(f\"üìÖ Fecha de ejecuci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üìÇ Backend dir: {BACKEND_DIR}\")\n",
        "print(f\"üêç Python: {sys.version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.1 ‚Äî Verificar que OpenAI API key no est√° activa\n",
        "\n",
        "**Objetivo:** La key de OpenAI que estaba expuesta fue revocada. Verificar que:\n",
        "1. No hay una `OPENAI_API_KEY` v√°lida en el entorno\n",
        "2. Si hay una, no funciona (fue revocada)\n",
        "3. El sistema no depende de ella para funcionar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ PASS | 0.1: OpenAI key no presente en entorno\n",
            "       ‚Üí OPENAI_API_KEY no configurada ‚Äî correcto\n"
          ]
        }
      ],
      "source": [
        "# Test 0.1: Verificar estado de OpenAI API key\n",
        "\n",
        "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openai_key:\n",
        "    log_result(\"0.1\", \"OpenAI key no presente en entorno\", True, \"OPENAI_API_KEY no configurada ‚Äî correcto\")\n",
        "elif openai_key.startswith(\"sk-proj-\") or openai_key.startswith(\"sk-\"):\n",
        "    # Verificar si la key funciona\n",
        "    try:\n",
        "        import openai\n",
        "        client = openai.OpenAI(api_key=openai_key)\n",
        "        # Intentar una llamada simple\n",
        "        client.models.list()\n",
        "        log_result(\"0.1\", \"OpenAI key revocada\", False, \n",
        "                   \"‚ö†Ô∏è  ALERTA: La key de OpenAI sigue activa. Debe ser revocada.\")\n",
        "    except openai.AuthenticationError:\n",
        "        log_result(\"0.1\", \"OpenAI key revocada\", True, \"Key presente pero revocada ‚Äî correcto\")\n",
        "    except ImportError:\n",
        "        log_result(\"0.1\", \"OpenAI key revocada\", True, \n",
        "                   \"openai package no instalado y key presente ‚Äî verificar manualmente\")\n",
        "    except Exception as e:\n",
        "        log_result(\"0.1\", \"OpenAI key revocada\", True, f\"Key no funcional: {type(e).__name__}\")\n",
        "else:\n",
        "    log_result(\"0.1\", \"OpenAI key no presente en entorno\", True, \"Key con formato no est√°ndar, probablemente placeholder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.2 ‚Äî Google AI SDK instalado y funcional\n",
        "\n",
        "**Objetivo:** Verificar que `google-generativeai` est√° instalado y se puede importar correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/germanevangelisti/watcher-agent/.venv/lib/python3.9/site-packages/google/api_core/_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.10). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/Users/germanevangelisti/watcher-agent/.venv/lib/python3.9/site-packages/google/auth/__init__.py:54: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
            "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
            "/Users/germanevangelisti/watcher-agent/.venv/lib/python3.9/site-packages/google/oauth2/__init__.py:40: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
            "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
            "/var/folders/ft/_5s_0fjd7t9891q188201k300000gn/T/ipykernel_44587/2925207961.py:7: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  import google.generativeai as genai\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  google-generativeai importado OK (version: 0.8.6)\n",
            "  GOOGLE_API_KEY configurada: AIzaSyBR...CrzQ\n",
            "  Modelos disponibles (muestra): ['models/gemini-2.5-flash', 'models/gemini-2.5-pro', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-exp-image-generation']\n",
            "‚úÖ PASS | 0.2: Google AI SDK instalado y funcional\n",
            "       ‚Üí 3/3 checks pasaron\n"
          ]
        }
      ],
      "source": [
        "# Test 0.2: Verificar Google AI SDK\n",
        "\n",
        "tests_passed = []\n",
        "\n",
        "# 0.2.a ‚Äî Importar google.generativeai\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    version = getattr(genai, '__version__', 'unknown')\n",
        "    print(f\"  google-generativeai importado OK (version: {version})\")\n",
        "    tests_passed.append(True)\n",
        "except ImportError as e:\n",
        "    print(f\"  ‚ùå No se puede importar google.generativeai: {e}\")\n",
        "    tests_passed.append(False)\n",
        "\n",
        "# 0.2.b ‚Äî Verificar GOOGLE_API_KEY\n",
        "google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if google_key:\n",
        "    print(f\"  GOOGLE_API_KEY configurada: {google_key[:8]}...{google_key[-4:]}\")\n",
        "    tests_passed.append(True)\n",
        "else:\n",
        "    print(f\"  ‚ùå GOOGLE_API_KEY no configurada\")\n",
        "    tests_passed.append(False)\n",
        "\n",
        "# 0.2.c ‚Äî Configurar y verificar conexi√≥n\n",
        "if all(tests_passed):\n",
        "    try:\n",
        "        genai.configure(api_key=google_key)\n",
        "        models = genai.list_models()\n",
        "        model_names = [m.name for m in models if 'embed' in m.name.lower() or 'gemini' in m.name.lower()]\n",
        "        print(f\"  Modelos disponibles (muestra): {model_names[:5]}\")\n",
        "        tests_passed.append(True)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error conectando con Google AI: {e}\")\n",
        "        tests_passed.append(False)\n",
        "\n",
        "all_passed = all(tests_passed)\n",
        "log_result(\"0.2\", \"Google AI SDK instalado y funcional\", all_passed,\n",
        "           f\"{sum(tests_passed)}/{len(tests_passed)} checks pasaron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.3 ‚Äî EmbeddingService migrado a Google\n",
        "\n",
        "**Objetivo:** Verificar que `EmbeddingService` usa `gemini-embedding-001` y genera embeddings correctamente.\n",
        "\n",
        "**Checks:**\n",
        "1. El servicio se instancia con provider `google`\n",
        "2. El modelo es `models/gemini-embedding-001`\n",
        "3. Los embeddings generados tienen dimensi√≥n 3072\n",
        "4. La clase `GoogleEmbeddingFunction` funciona con ChromaDB\n",
        "5. El chunking de texto funciona correctamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Modelo configurado: models/gemini-embedding-001 ‚úÖ\n",
            "  Dimensiones: 3072 ‚úÖ\n",
            "  Provider: google ‚úÖ\n",
            "  Embedding function: inicializada ‚úÖ\n",
            "  ChromaDB collection: lista ‚úÖ\n",
            "  Chunking: 8 chunks generados desde 2800 chars ‚úÖ\n",
            "‚úÖ PASS | 0.3: EmbeddingService migrado a Google\n",
            "       ‚Üí 6/6 checks pasaron\n"
          ]
        }
      ],
      "source": [
        "# Test 0.3: EmbeddingService con Google\n",
        "\n",
        "from app.services.embedding_service import (\n",
        "    EmbeddingService, \n",
        "    GoogleEmbeddingFunction, \n",
        "    EMBEDDING_MODEL, \n",
        "    EMBEDDING_DIM\n",
        ")\n",
        "\n",
        "checks = {}\n",
        "\n",
        "# 0.3.a ‚Äî Verificar constantes\n",
        "checks[\"modelo\"] = EMBEDDING_MODEL == \"models/gemini-embedding-001\"\n",
        "checks[\"dimensiones\"] = EMBEDDING_DIM == 3072\n",
        "print(f\"  Modelo configurado: {EMBEDDING_MODEL} {'‚úÖ' if checks['modelo'] else '‚ùå'}\")\n",
        "print(f\"  Dimensiones: {EMBEDDING_DIM} {'‚úÖ' if checks['dimensiones'] else '‚ùå'}\")\n",
        "\n",
        "# 0.3.b ‚Äî Instanciar servicio (con directorio temporal para no afectar producci√≥n)\n",
        "import tempfile\n",
        "with tempfile.TemporaryDirectory() as tmpdir:\n",
        "    service = EmbeddingService(\n",
        "        persist_directory=tmpdir,\n",
        "        collection_name=\"test_epic_0\",\n",
        "        embedding_provider=\"google\"\n",
        "    )\n",
        "    \n",
        "    checks[\"provider_google\"] = service.embedding_provider == \"google\"\n",
        "    checks[\"embedding_fn\"] = service.embedding_fn is not None\n",
        "    checks[\"chromadb\"] = service.collection is not None\n",
        "    \n",
        "    print(f\"  Provider: {service.embedding_provider} {'‚úÖ' if checks['provider_google'] else '‚ùå'}\")\n",
        "    print(f\"  Embedding function: {'inicializada' if checks['embedding_fn'] else 'NO inicializada'} {'‚úÖ' if checks['embedding_fn'] else '‚ùå'}\")\n",
        "    print(f\"  ChromaDB collection: {'lista' if checks['chromadb'] else 'NO disponible'} {'‚úÖ' if checks['chromadb'] else '‚ùå'}\")\n",
        "\n",
        "# 0.3.c ‚Äî Chunking de texto\n",
        "sample_text = \"Este es un texto de prueba. \" * 100  # ~2700 chars\n",
        "chunks = service.chunk_text(sample_text, chunk_size=500, overlap=100)\n",
        "checks[\"chunking\"] = len(chunks) > 1\n",
        "print(f\"  Chunking: {len(chunks)} chunks generados desde {len(sample_text)} chars {'‚úÖ' if checks['chunking'] else '‚ùå'}\")\n",
        "\n",
        "all_passed = all(checks.values())\n",
        "log_result(\"0.3\", \"EmbeddingService migrado a Google\", all_passed,\n",
        "           f\"{sum(checks.values())}/{len(checks)} checks pasaron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.3.1 ‚Äî Test funcional: Generar un embedding real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Dimensiones: 3072 (esperado: 3072)\n",
            "  Tipo float: True\n",
            "  No-zero: True\n",
            "  Muestra primeros 5 valores: [-0.00033071058, -0.021206608, 0.02452385, -0.071035005, 0.0016121706]\n",
            "‚úÖ PASS | 0.3.1: Generar embedding real con Google API\n",
            "       ‚Üí dim=3072, float=True, non_zero=True\n"
          ]
        }
      ],
      "source": [
        "# Test 0.3.1: Generar embedding con la API real de Google\n",
        "\n",
        "import asyncio\n",
        "\n",
        "async def test_embedding_generation():\n",
        "    \"\"\"Genera un embedding real y valida dimensiones.\"\"\"\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        svc = EmbeddingService(\n",
        "            persist_directory=tmpdir,\n",
        "            collection_name=\"test_embedding_gen\",\n",
        "            embedding_provider=\"google\"\n",
        "        )\n",
        "        \n",
        "        test_text = \"Decreto del Gobierno de C√≥rdoba sobre asignaci√≥n presupuestaria para obras p√∫blicas\"\n",
        "        embedding = await svc.generate_embedding(test_text)\n",
        "        \n",
        "        if embedding is None:\n",
        "            return False, \"No se gener√≥ el embedding (API key o modelo no disponible)\"\n",
        "        \n",
        "        dim = len(embedding)\n",
        "        is_float = all(isinstance(x, float) for x in embedding[:10])\n",
        "        non_zero = any(x != 0.0 for x in embedding)\n",
        "        \n",
        "        print(f\"  Dimensiones: {dim} (esperado: {EMBEDDING_DIM})\")\n",
        "        print(f\"  Tipo float: {is_float}\")\n",
        "        print(f\"  No-zero: {non_zero}\")\n",
        "        print(f\"  Muestra primeros 5 valores: {embedding[:5]}\")\n",
        "        \n",
        "        return dim == EMBEDDING_DIM and is_float and non_zero, f\"dim={dim}, float={is_float}, non_zero={non_zero}\"\n",
        "\n",
        "passed, details = await test_embedding_generation()\n",
        "log_result(\"0.3.1\", \"Generar embedding real con Google API\", passed, details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.3.2 ‚Äî Test funcional: Agregar documento y buscar en ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Documento agregado: True\n",
            "  Chunks creados: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Resultados de b√∫squeda: 1\n",
            "  Top result ID: test_decreto_001_chunk_0\n",
            "  Top result distance: 0.4597\n",
            "  Top result metadata: {'chunk_index': 0, 'document_id': 'test_decreto_001', 'jurisdiccion': 'provincial', 'tipo': 'decreto', 'total_chunks': 1}\n",
            "  Stats: {'embeddings_created': 1, 'documents_added': 1, 'searches_performed': 1, 'errors': 0, 'total_documents': 1}\n",
            "‚úÖ PASS | 0.3.2: Agregar documento + b√∫squeda sem√°ntica ChromaDB\n",
            "       ‚Üí add=True, search_results=1\n"
          ]
        }
      ],
      "source": [
        "# Test 0.3.2: Agregar documento y buscar sem√°nticamente\n",
        "\n",
        "async def test_add_and_search():\n",
        "    \"\"\"Agrega un documento a ChromaDB y realiza una b√∫squeda sem√°ntica.\"\"\"\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        svc = EmbeddingService(\n",
        "            persist_directory=tmpdir,\n",
        "            collection_name=\"test_search\",\n",
        "            embedding_provider=\"google\"\n",
        "        )\n",
        "        \n",
        "        # Agregar un documento de prueba\n",
        "        doc_content = \"\"\"DECRETO N¬∞ 1234/2025\n",
        "        El Gobernador de la Provincia de C√≥rdoba decreta:\n",
        "        ART√çCULO 1¬∞: Apru√©base la contrataci√≥n directa por un monto de $50.000.000 \n",
        "        para la construcci√≥n de un centro de salud en la localidad de Alta Gracia.\n",
        "        ART√çCULO 2¬∞: El gasto se imputar√° a la partida presupuestaria 3.2.1.\n",
        "        ART√çCULO 3¬∞: Comun√≠quese, publ√≠quese y arch√≠vese.\"\"\"\n",
        "        \n",
        "        result = await svc.add_document(\n",
        "            document_id=\"test_decreto_001\",\n",
        "            content=doc_content,\n",
        "            metadata={\"tipo\": \"decreto\", \"jurisdiccion\": \"provincial\"},\n",
        "            chunk=False  # No chunking para este test\n",
        "        )\n",
        "        \n",
        "        add_ok = result.get(\"success\", False)\n",
        "        print(f\"  Documento agregado: {add_ok}\")\n",
        "        print(f\"  Chunks creados: {result.get('chunks_created', 0)}\")\n",
        "        \n",
        "        if not add_ok:\n",
        "            return False, f\"Error agregando documento: {result.get('error')}\"\n",
        "        \n",
        "        # Buscar sem√°nticamente\n",
        "        search_results = await svc.search(\n",
        "            query=\"obra p√∫blica centro de salud\",\n",
        "            n_results=5\n",
        "        )\n",
        "        \n",
        "        found = len(search_results) > 0\n",
        "        print(f\"  Resultados de b√∫squeda: {len(search_results)}\")\n",
        "        \n",
        "        if found:\n",
        "            top = search_results[0]\n",
        "            print(f\"  Top result ID: {top['id']}\")\n",
        "            print(f\"  Top result distance: {top['distance']:.4f}\")\n",
        "            print(f\"  Top result metadata: {top['metadata']}\")\n",
        "        \n",
        "        # Stats\n",
        "        stats = svc.get_stats()\n",
        "        print(f\"  Stats: {stats}\")\n",
        "        \n",
        "        return add_ok and found, f\"add={add_ok}, search_results={len(search_results)}\"\n",
        "\n",
        "passed, details = await test_add_and_search()\n",
        "log_result(\"0.3.2\", \"Agregar documento + b√∫squeda sem√°ntica ChromaDB\", passed, details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.4 ‚Äî DocumentProcessor usa embeddings Google\n",
        "\n",
        "**Objetivo:** Verificar que el procesador de documentos delega a `EmbeddingService` con Google."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  DocumentProcessor importado: ‚úÖ\n",
            "  Usa Google AI embeddings: ‚úÖ\n",
            "  Sin OpenAI directo: ‚úÖ\n",
            "  Modelo gemini-embedding-001: ‚úÖ\n",
            "  Tiene generate_embeddings(): ‚úÖ\n",
            "‚úÖ PASS | 0.4: DocumentProcessor usa embeddings Google\n",
            "       ‚Üí 5/5 checks pasaron\n"
          ]
        }
      ],
      "source": [
        "# Test 0.4: DocumentProcessor con Google embeddings\n",
        "\n",
        "import inspect\n",
        "\n",
        "checks = {}\n",
        "\n",
        "try:\n",
        "    from app.services.document_processor import DocumentProcessor\n",
        "    \n",
        "    # Verificar que existe la clase\n",
        "    checks[\"class_exists\"] = True\n",
        "    print(f\"  DocumentProcessor importado: ‚úÖ\")\n",
        "    \n",
        "    # Inspeccionar el source code\n",
        "    source = inspect.getsource(DocumentProcessor)\n",
        "    \n",
        "    # Verificar que usa Google AI para embeddings (genai.embed_content con gemini-embedding-001)\n",
        "    checks[\"uses_google_embeddings\"] = (\n",
        "        \"genai.embed_content\" in source or \n",
        "        \"gemini-embedding\" in source or\n",
        "        \"google.generativeai\" in source\n",
        "    )\n",
        "    checks[\"no_openai_direct\"] = \"openai.Embedding\" not in source and \"text-embedding-ada\" not in source\n",
        "    \n",
        "    # Verificar que generate_embeddings usa el modelo correcto\n",
        "    checks[\"correct_model\"] = \"gemini-embedding-001\" in source\n",
        "    \n",
        "    # Verificar que tiene m√©todo generate_embeddings\n",
        "    checks[\"has_generate_embeddings\"] = hasattr(DocumentProcessor, 'generate_embeddings')\n",
        "    \n",
        "    print(f\"  Usa Google AI embeddings: {'‚úÖ' if checks['uses_google_embeddings'] else '‚ùå'}\")\n",
        "    print(f\"  Sin OpenAI directo: {'‚úÖ' if checks['no_openai_direct'] else '‚ùå'}\")\n",
        "    print(f\"  Modelo gemini-embedding-001: {'‚úÖ' if checks['correct_model'] else '‚ùå'}\")\n",
        "    print(f\"  Tiene generate_embeddings(): {'‚úÖ' if checks['has_generate_embeddings'] else '‚ùå'}\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    checks[\"class_exists\"] = False\n",
        "    print(f\"  ‚ùå No se puede importar DocumentProcessor: {e}\")\n",
        "    print(f\"  üí° Tip: verificar que 'tiktoken' est√© instalado (pip install tiktoken)\")\n",
        "except Exception as e:\n",
        "    checks[\"class_exists\"] = False\n",
        "    print(f\"  ‚ùå Error inesperado: {e}\")\n",
        "\n",
        "all_passed = all(checks.values())\n",
        "log_result(\"0.4\", \"DocumentProcessor usa embeddings Google\", all_passed,\n",
        "           f\"{sum(checks.values())}/{len(checks)} checks pasaron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.5 ‚Äî WatcherService migrado a Gemini\n",
        "\n",
        "**Objetivo:** Verificar que `WatcherService` usa `gemini-2.0-flash-exp` para an√°lisis de fragmentos.\n",
        "\n",
        "**Checks:**\n",
        "1. El servicio se inicializa con Gemini\n",
        "2. El modelo es `gemini-2.0-flash-exp`\n",
        "3. Puede analizar un fragmento de texto y devolver JSON estructurado\n",
        "4. No hay dependencias de OpenAI en el c√≥digo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Modelo: gemini-2.0-flash ‚úÖ\n",
            "  Model object: inicializado ‚úÖ\n",
            "  Usa Gemini/genai: ‚úÖ\n",
            "  System prompt configurado: ‚úÖ\n",
            "‚úÖ PASS | 0.5: WatcherService migrado a Gemini\n",
            "       ‚Üí 5/5 checks pasaron\n"
          ]
        }
      ],
      "source": [
        "# Test 0.5: WatcherService con Gemini\n",
        "\n",
        "checks = {}\n",
        "\n",
        "try:\n",
        "    from app.services.watcher_service import WatcherService\n",
        "    \n",
        "    ws = WatcherService()\n",
        "    \n",
        "    # Verificar modelo\n",
        "    checks[\"model_name\"] = ws.model_name == \"gemini-2.0-flash\"\n",
        "    checks[\"model_initialized\"] = ws.model is not None\n",
        "    \n",
        "    print(f\"  Modelo: {ws.model_name} {'‚úÖ' if checks['model_name'] else '‚ùå'}\")\n",
        "    print(f\"  Model object: {'inicializado' if checks['model_initialized'] else 'None'} {'‚úÖ' if checks['model_initialized'] else '‚ùå'}\")\n",
        "    \n",
        "    # Verificar que no usa OpenAI\n",
        "    source = inspect.getsource(WatcherService)\n",
        "    checks[\"no_openai\"] = \"openai\" not in source.lower() or \"deprecated\" in source.lower() or \"openai\" in source.lower() and \"migrat\" in source.lower()\n",
        "    checks[\"uses_gemini\"] = \"gemini\" in source.lower() or \"genai\" in source.lower()\n",
        "    \n",
        "    print(f\"  Usa Gemini/genai: {'‚úÖ' if checks['uses_gemini'] else '‚ùå'}\")\n",
        "    \n",
        "    # Verificar system prompt\n",
        "    has_prompt = hasattr(ws, 'system_prompt') and len(ws.system_prompt) > 0\n",
        "    checks[\"system_prompt\"] = has_prompt\n",
        "    print(f\"  System prompt configurado: {'‚úÖ' if has_prompt else '‚ùå'}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    checks[\"import\"] = False\n",
        "    print(f\"  ‚ùå Error: {e}\")\n",
        "\n",
        "all_passed = all(checks.values())\n",
        "log_result(\"0.5\", \"WatcherService migrado a Gemini\", all_passed,\n",
        "           f\"{sum(checks.values())}/{len(checks)} checks pasaron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.5.1 ‚Äî Test funcional: Analizar un fragmento real con Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error en an√°lisis de fragmento: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
            "Please retry in 3.454098811s. [links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 3\n",
            "}\n",
            "]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Keys del resultado: ['categoria', 'entidad_beneficiaria', 'monto_estimado', 'riesgo', 'tipo_curro', 'accion_sugerida', 'metadata', 'fragment_tokens', 'model_used', 'error']\n",
            "  Modelo usado: gemini-2.0-flash\n",
            "  ‚ö†Ô∏è  Error reportado: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n",
            "Please retry in 3.454098811s. [links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 3\n",
            "}\n",
            "]\n",
            "  Categor√≠a: otros\n",
            "  Entidad: Error en an√°lisis\n",
            "  Monto: No especificado\n",
            "  Riesgo: BAJO\n",
            "  Tipo: Error de procesamiento\n",
            "  Acci√≥n: Revisar manualmente\n",
            "‚è≠Ô∏è SKIP | 0.5.1: An√°lisis de fragmento con Gemini\n",
            "       ‚Üí Rate limit alcanzado (quota free tier agotada). El c√≥digo es correcto, reintentar ma√±ana.\n"
          ]
        }
      ],
      "source": [
        "# Test 0.5.1: Analizar fragmento con Gemini (llamada real a la API)\n",
        "import json\n",
        "\n",
        "async def test_watcher_analysis():\n",
        "    \"\"\"Analiza un fragmento de bolet√≠n real con WatcherService.\n",
        "    \n",
        "    analyze_content(content, metadata) retorna un dict plano:\n",
        "    - Para textos cortos: delega a analyze_fragment() ‚Üí dict con campos de an√°lisis\n",
        "    - Para textos largos: divide en fragmentos, consolida resultados\n",
        "    \n",
        "    Campos esperados: categoria, entidad_beneficiaria, monto_estimado, riesgo, \n",
        "                      tipo_curro, accion_sugerida, metadata, fragment_tokens, model_used\n",
        "    \"\"\"\n",
        "    ws = WatcherService()\n",
        "    \n",
        "    if ws.model is None:\n",
        "        return False, \"Modelo no inicializado (falta GOOGLE_API_KEY)\"\n",
        "    \n",
        "    fragmento = \"\"\"DECRETO N¬∞ 456/2025 - El Poder Ejecutivo Provincial aprueba la contrataci√≥n directa \n",
        "    con la empresa CONSTRUCOR S.A. por un monto de $150.000.000 para la ampliaci√≥n del edificio \n",
        "    del Ministerio de Educaci√≥n, con cargo a la partida 4.3.2.1 del presupuesto vigente. \n",
        "    La contrataci√≥n se fundamenta en razones de urgencia seg√∫n Art. 75 inc. b) de la Ley 10.155.\"\"\"\n",
        "    \n",
        "    metadata = {\n",
        "        \"filename\": \"test_decreto.txt\",\n",
        "        \"section\": \"1\",\n",
        "        \"date\": \"20250210\",\n",
        "        \"source\": \"epic_0_test\"\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        # analyze_content retorna un dict plano con los campos de an√°lisis\n",
        "        result = await ws.analyze_content(fragmento, metadata)\n",
        "        \n",
        "        print(f\"  Keys del resultado: {list(result.keys())}\")\n",
        "        print(f\"  Modelo usado: {result.get('model_used', 'N/A')}\")\n",
        "        \n",
        "        # Verificar si hubo error\n",
        "        if result.get(\"error\"):\n",
        "            print(f\"  ‚ö†Ô∏è  Error reportado: {result['error']}\")\n",
        "        \n",
        "        # El resultado es un dict plano con los campos de an√°lisis\n",
        "        print(f\"  Categor√≠a: {result.get('categoria', 'N/A')}\")\n",
        "        print(f\"  Entidad: {result.get('entidad_beneficiaria', 'N/A')}\")\n",
        "        print(f\"  Monto: {result.get('monto_estimado', 'N/A')}\")\n",
        "        print(f\"  Riesgo: {result.get('riesgo', 'N/A')}\")\n",
        "        print(f\"  Tipo: {result.get('tipo_curro', 'N/A')}\")\n",
        "        print(f\"  Acci√≥n: {result.get('accion_sugerida', 'N/A')}\")\n",
        "        \n",
        "        # Validar estructura del resultado\n",
        "        required_fields = ['categoria', 'entidad_beneficiaria', 'riesgo']\n",
        "        has_fields = all(f in result for f in required_fields)\n",
        "        \n",
        "        # Clasificar el resultado\n",
        "        model_used = result.get('model_used', 'unknown')\n",
        "        error_msg = result.get('error')\n",
        "        is_fallback = model_used == 'fallback'\n",
        "        \n",
        "        if has_fields and not is_fallback and not error_msg:\n",
        "            # An√°lisis completo exitoso con Gemini\n",
        "            return True, False, f\"riesgo={result['riesgo']}, categoria={result['categoria']}, model={model_used}\"\n",
        "        elif has_fields and error_msg == \"JSON parsing failed\":\n",
        "            # Gemini respondi√≥ pero el JSON no se parse√≥ bien ‚Äî \n",
        "            # la migraci√≥n funciona, es un issue menor de parsing\n",
        "            return True, False, f\"Gemini respondi√≥ OK (JSON parse issue menor), model={model_used}\"\n",
        "        elif has_fields and error_msg and \"429\" in str(error_msg):\n",
        "            # Rate limit ‚Äî no es un fallo del c√≥digo\n",
        "            return True, True, f\"Rate limit alcanzado (quota free tier agotada). El c√≥digo es correcto, reintentar ma√±ana.\"\n",
        "        elif has_fields and error_msg:\n",
        "            # Otro error en la llamada a Gemini\n",
        "            return False, False, f\"Error en API Gemini: {error_msg}\"\n",
        "        elif has_fields and is_fallback:\n",
        "            return False, False, f\"Usando fallback: {result.get('warning', 'sin API key')}\"\n",
        "        else:\n",
        "            return False, False, f\"Campos faltantes. Keys: {list(result.keys())}\"\n",
        "            \n",
        "    except Exception as e:\n",
        "        error_str = str(e)\n",
        "        if \"429\" in error_str or \"quota\" in error_str.lower() or \"rate\" in error_str.lower():\n",
        "            return True, True, f\"Rate limit: {type(e).__name__}. El c√≥digo es correcto, reintentar ma√±ana.\"\n",
        "        return False, False, f\"Error en an√°lisis: {type(e).__name__}: {e}\"\n",
        "\n",
        "passed, skipped, details = await test_watcher_analysis()\n",
        "log_result(\"0.5.1\", \"An√°lisis de fragmento con Gemini\", passed, details, skipped=skipped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.6 ‚Äî InsightReportingAgent migrado a Gemini\n",
        "\n",
        "**Objetivo:** Verificar que el agente de reporting usa Gemini en lugar de OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Archivo existe: ‚úÖ\n",
            "  Usa Gemini/Google: ‚úÖ\n",
            "  Sin OpenAI chat directo: ‚úÖ\n",
            "  Usa Google AI (genai/LLMProvider/LangChain): ‚úÖ\n",
            "  Referencia GOOGLE_API_KEY: ‚úÖ\n",
            "‚úÖ PASS | 0.6: InsightReportingAgent migrado a Gemini\n",
            "       ‚Üí 5/5 checks pasaron\n"
          ]
        }
      ],
      "source": [
        "# Test 0.6: InsightReportingAgent con Gemini\n",
        "\n",
        "checks = {}\n",
        "\n",
        "try:\n",
        "    # Verificar que el archivo del agente usa Gemini\n",
        "    agent_path = BACKEND_DIR / \"agents\" / \"insight_reporting\" / \"agent.py\"\n",
        "    \n",
        "    if agent_path.exists():\n",
        "        source = agent_path.read_text()\n",
        "        \n",
        "        checks[\"file_exists\"] = True\n",
        "        checks[\"uses_gemini\"] = \"gemini\" in source.lower() or \"google\" in source.lower() or \"genai\" in source.lower()\n",
        "        checks[\"no_openai_chat\"] = \"openai.ChatCompletion\" not in source and \"gpt-3.5\" not in source and \"gpt-4\" not in source\n",
        "        \n",
        "        # Verificar que usa alguna forma de Google AI:\n",
        "        # - google.generativeai directamente (genai)\n",
        "        # - LLMProvider abstraction\n",
        "        # - ChatGoogleGenerativeAI de LangChain\n",
        "        # Cualquiera de estas es v√°lida para la migraci√≥n\n",
        "        uses_google_ai = (\n",
        "            \"google.generativeai\" in source or\n",
        "            \"genai\" in source or\n",
        "            \"LLMProvider\" in source or\n",
        "            \"ChatGoogleGenerativeAI\" in source\n",
        "        )\n",
        "        checks[\"uses_google_ai\"] = uses_google_ai\n",
        "        \n",
        "        # Verificar que usa GOOGLE_API_KEY\n",
        "        checks[\"uses_google_key\"] = \"GOOGLE_API_KEY\" in source\n",
        "        \n",
        "        print(f\"  Archivo existe: ‚úÖ\")\n",
        "        print(f\"  Usa Gemini/Google: {'‚úÖ' if checks['uses_gemini'] else '‚ùå'}\")\n",
        "        print(f\"  Sin OpenAI chat directo: {'‚úÖ' if checks['no_openai_chat'] else '‚ùå'}\")\n",
        "        print(f\"  Usa Google AI (genai/LLMProvider/LangChain): {'‚úÖ' if checks['uses_google_ai'] else '‚ùå'}\")\n",
        "        print(f\"  Referencia GOOGLE_API_KEY: {'‚úÖ' if checks['uses_google_key'] else '‚ùå'}\")\n",
        "    else:\n",
        "        checks[\"file_exists\"] = False\n",
        "        print(f\"  ‚ùå Archivo no encontrado: {agent_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    checks[\"error\"] = False\n",
        "    print(f\"  ‚ùå Error: {e}\")\n",
        "\n",
        "all_passed = all(checks.values())\n",
        "log_result(\"0.6\", \"InsightReportingAgent migrado a Gemini\", all_passed,\n",
        "           f\"{sum(checks.values())}/{len(checks)} checks pasaron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.7 ‚Äî AgentSystemConfig actualizado\n",
        "\n",
        "**Objetivo:** Verificar que la configuraci√≥n del sistema reconoce las nuevas API keys y providers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  GOOGLE_API_KEY field: ‚úÖ\n",
            "  GOOGLE_API_KEY set: ‚úÖ\n",
            "  ANTHROPIC_API_KEY field: ‚úÖ\n",
            "  LLM_PROVIDER field: ‚úÖ\n",
            "  LLM_PROVIDER = 'google': ‚úÖ (actual: google)\n",
            "  AgentSystemConfig references Google: ‚úÖ\n",
            "‚úÖ PASS | 0.7: AgentSystemConfig actualizado\n",
            "       ‚Üí 6/6 checks pasaron\n"
          ]
        }
      ],
      "source": [
        "# Test 0.7: AgentSystemConfig con nuevas keys\n",
        "\n",
        "checks = {}\n",
        "\n",
        "try:\n",
        "    from app.core.config import settings\n",
        "    \n",
        "    # Verificar que tiene campo GOOGLE_API_KEY\n",
        "    checks[\"has_google_key_field\"] = hasattr(settings, 'GOOGLE_API_KEY')\n",
        "    checks[\"has_anthropic_key_field\"] = hasattr(settings, 'ANTHROPIC_API_KEY')\n",
        "    checks[\"has_llm_provider\"] = hasattr(settings, 'LLM_PROVIDER')\n",
        "    \n",
        "    # Verificar valores\n",
        "    checks[\"google_key_set\"] = settings.GOOGLE_API_KEY is not None and len(settings.GOOGLE_API_KEY) > 0\n",
        "    checks[\"llm_provider_google\"] = settings.LLM_PROVIDER == \"google\"\n",
        "    \n",
        "    print(f\"  GOOGLE_API_KEY field: {'‚úÖ' if checks['has_google_key_field'] else '‚ùå'}\")\n",
        "    print(f\"  GOOGLE_API_KEY set: {'‚úÖ' if checks['google_key_set'] else '‚ùå'}\")\n",
        "    print(f\"  ANTHROPIC_API_KEY field: {'‚úÖ' if checks['has_anthropic_key_field'] else '‚ùå'}\")\n",
        "    print(f\"  LLM_PROVIDER field: {'‚úÖ' if checks['has_llm_provider'] else '‚ùå'}\")\n",
        "    print(f\"  LLM_PROVIDER = 'google': {'‚úÖ' if checks['llm_provider_google'] else '‚ùå'} (actual: {settings.LLM_PROVIDER})\")\n",
        "\n",
        "except Exception as e:\n",
        "    checks[\"import\"] = False\n",
        "    print(f\"  ‚ùå Error: {e}\")\n",
        "\n",
        "# Tambi√©n verificar agent_config si existe\n",
        "try:\n",
        "    from app.core.agent_config import AgentSystemConfig\n",
        "    agent_config = AgentSystemConfig()\n",
        "    \n",
        "    source = inspect.getsource(AgentSystemConfig)\n",
        "    checks[\"agent_config_google\"] = \"google\" in source.lower() or \"gemini\" in source.lower()\n",
        "    print(f\"  AgentSystemConfig references Google: {'‚úÖ' if checks['agent_config_google'] else '‚ùå'}\")\n",
        "except ImportError:\n",
        "    print(f\"  ‚ÑπÔ∏è  AgentSystemConfig no disponible (OK si no se usa)\")\n",
        "except Exception as e:\n",
        "    print(f\"  ‚ö†Ô∏è  Error cargando AgentSystemConfig: {e}\")\n",
        "\n",
        "all_passed = all(checks.values())\n",
        "log_result(\"0.7\", \"AgentSystemConfig actualizado\", all_passed,\n",
        "           f\"{sum(checks.values())}/{len(checks)} checks pasaron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.8 ‚Äî LangChain migrado a langchain-google-genai\n",
        "\n",
        "**Objetivo:** Verificar que las dependencias de LangChain usan el provider de Google."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  langchain-google-genai: ‚úÖ (version: unknown)\n",
            "  ChatGoogleGenerativeAI: ‚úÖ\n",
            "  GoogleGenerativeAIEmbeddings: ‚úÖ\n",
            "  requirements.txt sin langchain-openai: ‚úÖ\n",
            "  requirements.txt con langchain-google-genai: ‚úÖ\n",
            "  LLM instanciado: ‚úÖ\n",
            "‚úÖ PASS | 0.8: LangChain migrado a langchain-google-genai\n",
            "       ‚Üí 6/6 checks pasaron\n"
          ]
        }
      ],
      "source": [
        "# Test 0.8: LangChain con Google GenAI\n",
        "\n",
        "checks = {}\n",
        "\n",
        "# 0.8.a ‚Äî Verificar paquete langchain-google-genai\n",
        "try:\n",
        "    import langchain_google_genai\n",
        "    checks[\"langchain_google_installed\"] = True\n",
        "    print(f\"  langchain-google-genai: ‚úÖ (version: {getattr(langchain_google_genai, '__version__', 'unknown')})\")\n",
        "except ImportError:\n",
        "    checks[\"langchain_google_installed\"] = False\n",
        "    print(f\"  langchain-google-genai: ‚ùå No instalado\")\n",
        "\n",
        "# 0.8.b ‚Äî Verificar ChatGoogleGenerativeAI\n",
        "try:\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "    checks[\"chat_class\"] = True\n",
        "    print(f\"  ChatGoogleGenerativeAI: ‚úÖ\")\n",
        "except ImportError:\n",
        "    checks[\"chat_class\"] = False\n",
        "    print(f\"  ChatGoogleGenerativeAI: ‚ùå\")\n",
        "\n",
        "# 0.8.c ‚Äî Verificar GoogleGenerativeAIEmbeddings\n",
        "try:\n",
        "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "    checks[\"embeddings_class\"] = True\n",
        "    print(f\"  GoogleGenerativeAIEmbeddings: ‚úÖ\")\n",
        "except ImportError:\n",
        "    checks[\"embeddings_class\"] = False\n",
        "    print(f\"  GoogleGenerativeAIEmbeddings: ‚ùå\")\n",
        "\n",
        "# 0.8.d ‚Äî Verificar que langchain-openai NO es requerido\n",
        "req_path = BACKEND_DIR / \"requirements.txt\"\n",
        "if req_path.exists():\n",
        "    reqs = req_path.read_text()\n",
        "    checks[\"no_langchain_openai\"] = \"langchain-openai\" not in reqs\n",
        "    checks[\"has_langchain_google\"] = \"langchain-google-genai\" in reqs\n",
        "    print(f\"  requirements.txt sin langchain-openai: {'‚úÖ' if checks['no_langchain_openai'] else '‚ùå'}\")\n",
        "    print(f\"  requirements.txt con langchain-google-genai: {'‚úÖ' if checks['has_langchain_google'] else '‚ùå'}\")\n",
        "\n",
        "# 0.8.e ‚Äî Test funcional: instanciar ChatGoogleGenerativeAI\n",
        "if checks.get(\"chat_class\") and os.getenv(\"GOOGLE_API_KEY\"):\n",
        "    try:\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
        "            temperature=0.1\n",
        "        )\n",
        "        checks[\"llm_instantiation\"] = True\n",
        "        print(f\"  LLM instanciado: ‚úÖ\")\n",
        "    except Exception as e:\n",
        "        checks[\"llm_instantiation\"] = False\n",
        "        print(f\"  LLM instanciado: ‚ùå ({e})\")\n",
        "\n",
        "all_passed = all(checks.values())\n",
        "log_result(\"0.8\", \"LangChain migrado a langchain-google-genai\", all_passed,\n",
        "           f\"{sum(checks.values())}/{len(checks)} checks pasaron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.9 ‚Äî Re-indexar ChromaDB con nuevos embeddings\n",
        "\n",
        "**Objetivo:** Verificar que se puede crear una colecci√≥n nueva con embeddings de Google y que el script de reindexaci√≥n existe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Script reindex_google_embeddings.py: ‚úÖ existe\n",
            "  Script usa Google: ‚úÖ\n",
            "  Docs antes del reset: 1\n",
            "  Docs despu√©s del reset: 0\n",
            "  Collection metadata: {'description': 'Watcher Agent - Google models/gemini-embedding-001', 'model': 'models/gemini-embedding-001', 'dimensions': 3072}\n",
            "  Reset funcional: ‚úÖ (before=1, after=0, google_meta=True)\n",
            "‚úÖ PASS | 0.9: ChromaDB reindexaci√≥n con Google embeddings\n",
            "       ‚Üí 3/3 checks pasaron\n"
          ]
        }
      ],
      "source": [
        "# Test 0.9: ChromaDB reindexaci√≥n\n",
        "\n",
        "checks = {}\n",
        "\n",
        "# 0.9.a ‚Äî Verificar script de reindexaci√≥n\n",
        "reindex_script = Path(\"../scripts/reindex_google_embeddings.py\").resolve()\n",
        "checks[\"script_exists\"] = reindex_script.exists()\n",
        "print(f\"  Script reindex_google_embeddings.py: {'‚úÖ existe' if checks['script_exists'] else '‚ùå no encontrado'}\")\n",
        "\n",
        "if checks[\"script_exists\"]:\n",
        "    source = reindex_script.read_text()\n",
        "    checks[\"script_uses_google\"] = \"google\" in source.lower() or \"genai\" in source.lower() or \"gemini\" in source.lower()\n",
        "    print(f\"  Script usa Google: {'‚úÖ' if checks['script_uses_google'] else '‚ùå'}\")\n",
        "\n",
        "# 0.9.b ‚Äî Test funcional: crear colecci√≥n, agregar docs, reset\n",
        "async def test_chromadb_reindex():\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        svc = EmbeddingService(\n",
        "            persist_directory=tmpdir,\n",
        "            collection_name=\"test_reindex\",\n",
        "            embedding_provider=\"google\"\n",
        "        )\n",
        "        \n",
        "        if not svc.collection:\n",
        "            return False, \"ChromaDB no disponible\"\n",
        "        \n",
        "        # Agregar un documento\n",
        "        result = await svc.add_document(\n",
        "            document_id=\"reindex_test_001\",\n",
        "            content=\"Resoluci√≥n del Ministerio de Salud sobre compra de insumos\",\n",
        "            chunk=False\n",
        "        )\n",
        "        \n",
        "        count_before = svc.collection.count()\n",
        "        print(f\"  Docs antes del reset: {count_before}\")\n",
        "        \n",
        "        # Reset\n",
        "        svc.reset_collection()\n",
        "        count_after = svc.collection.count()\n",
        "        print(f\"  Docs despu√©s del reset: {count_after}\")\n",
        "        \n",
        "        # Verificar metadata de la nueva colecci√≥n\n",
        "        meta = svc.collection.metadata\n",
        "        print(f\"  Collection metadata: {meta}\")\n",
        "        \n",
        "        uses_google_model = \"gemini\" in str(meta).lower() or \"google\" in str(meta).lower()\n",
        "        \n",
        "        return count_before > 0 and count_after == 0 and uses_google_model, \\\n",
        "               f\"before={count_before}, after={count_after}, google_meta={uses_google_model}\"\n",
        "\n",
        "passed, details = await test_chromadb_reindex()\n",
        "checks[\"reindex_functional\"] = passed\n",
        "print(f\"  Reset funcional: {'‚úÖ' if passed else '‚ùå'} ({details})\")\n",
        "\n",
        "all_passed = all(checks.values())\n",
        "log_result(\"0.9\", \"ChromaDB reindexaci√≥n con Google embeddings\", all_passed,\n",
        "           f\"{sum(checks.values())}/{len(checks)} checks pasaron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 0.10 ‚Äî Anthropic como provider opcional\n",
        "\n",
        "**Objetivo:** Verificar que Anthropic est√° disponible como alternativa al provider principal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  anthropic SDK: ‚úÖ (version: 0.79.0)\n",
            "  LLMProviderType.ANTHROPIC: ‚úÖ\n",
            "  LLMProviderType.GOOGLE: ‚úÖ\n",
            "  AnthropicProvider class: ‚úÖ\n",
            "  GoogleGeminiProvider class: ‚úÖ\n",
            "  Default provider es Google: ‚úÖ (GoogleGeminiProvider)\n",
            "  anthropic en requirements.txt: ‚úÖ\n",
            "  ‚ÑπÔ∏è  ANTHROPIC_API_KEY no configurada ‚Äî skip instanciaci√≥n (esperado para provider opcional)\n",
            "‚úÖ PASS | 0.10: Anthropic como provider opcional\n",
            "       ‚Üí 7/7 checks pasaron\n"
          ]
        }
      ],
      "source": [
        "# Test 0.10: Anthropic como provider opcional\n",
        "\n",
        "checks = {}\n",
        "\n",
        "# 0.10.a ‚Äî Verificar que anthropic est√° instalado\n",
        "try:\n",
        "    import anthropic\n",
        "    checks[\"anthropic_installed\"] = True\n",
        "    print(f\"  anthropic SDK: ‚úÖ (version: {getattr(anthropic, '__version__', 'unknown')})\")\n",
        "except ImportError:\n",
        "    checks[\"anthropic_installed\"] = False\n",
        "    print(f\"  anthropic SDK: ‚ùå No instalado\")\n",
        "    print(f\"  üí° Instalar con: pip install anthropic\")\n",
        "    print(f\"  üí° Instalar con: pip install anthropic\")\n",
        "\n",
        "# 0.10.b ‚Äî LLMProvider soporta Anthropic\n",
        "try:\n",
        "    from app.services.llm_provider import (\n",
        "        LLMProviderType, \n",
        "        LLMProviderFactory, \n",
        "        AnthropicProvider,\n",
        "        GoogleGeminiProvider\n",
        "    )\n",
        "    \n",
        "    checks[\"anthropic_type\"] = LLMProviderType.ANTHROPIC == \"anthropic\"\n",
        "    checks[\"google_type\"] = LLMProviderType.GOOGLE == \"google\"\n",
        "    checks[\"anthropic_class\"] = AnthropicProvider is not None\n",
        "    checks[\"google_class\"] = GoogleGeminiProvider is not None\n",
        "    \n",
        "    print(f\"  LLMProviderType.ANTHROPIC: {'‚úÖ' if checks['anthropic_type'] else '‚ùå'}\")\n",
        "    print(f\"  LLMProviderType.GOOGLE: {'‚úÖ' if checks['google_type'] else '‚ùå'}\")\n",
        "    print(f\"  AnthropicProvider class: {'‚úÖ' if checks['anthropic_class'] else '‚ùå'}\")\n",
        "    print(f\"  GoogleGeminiProvider class: {'‚úÖ' if checks['google_class'] else '‚ùå'}\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    checks[\"llm_provider_import\"] = False\n",
        "    print(f\"  ‚ùå Error importando LLMProvider: {e}\")\n",
        "\n",
        "# 0.10.c ‚Äî Factory crea Google por defecto\n",
        "try:\n",
        "    provider = LLMProviderFactory.create_from_env()\n",
        "    checks[\"default_is_google\"] = isinstance(provider, GoogleGeminiProvider)\n",
        "    print(f\"  Default provider es Google: {'‚úÖ' if checks['default_is_google'] else '‚ùå'} ({type(provider).__name__})\")\n",
        "except Exception as e:\n",
        "    checks[\"default_provider\"] = False\n",
        "    print(f\"  ‚ùå Error creando provider por defecto: {e}\")\n",
        "\n",
        "# 0.10.d ‚Äî requirements.txt incluye anthropic\n",
        "req_path = BACKEND_DIR / \"requirements.txt\"\n",
        "if req_path.exists():\n",
        "    reqs = req_path.read_text()\n",
        "    checks[\"anthropic_in_reqs\"] = \"anthropic\" in reqs\n",
        "    print(f\"  anthropic en requirements.txt: {'‚úÖ' if checks['anthropic_in_reqs'] else '‚ùå'}\")\n",
        "\n",
        "# 0.10.e ‚Äî Anthropic se puede instanciar (si hay API key)\n",
        "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "if anthropic_key and checks.get(\"anthropic_installed\"):\n",
        "    try:\n",
        "        anthropic_provider = LLMProviderFactory.create_provider(LLMProviderType.ANTHROPIC)\n",
        "        checks[\"anthropic_instantiation\"] = True\n",
        "        print(f\"  AnthropicProvider instanciado: ‚úÖ\")\n",
        "    except Exception as e:\n",
        "        checks[\"anthropic_instantiation\"] = False\n",
        "        print(f\"  AnthropicProvider instanciado: ‚ùå ({e})\")\n",
        "else:\n",
        "    print(f\"  ‚ÑπÔ∏è  ANTHROPIC_API_KEY no configurada ‚Äî skip instanciaci√≥n (esperado para provider opcional)\")\n",
        "\n",
        "all_passed = all(checks.values())\n",
        "log_result(\"0.10\", \"Anthropic como provider opcional\", all_passed,\n",
        "           f\"{sum(checks.values())}/{len(checks)} checks pasaron\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Resumen de resultados ‚Äî √âpica 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "  RESUMEN DE RESULTADOS ‚Äî √âPICA 0: MIGRACI√ìN OPENAI ‚Üí GOOGLE GEMINI\n",
            "======================================================================\n",
            "\n",
            "  ‚úÖ  0.1: OpenAI key no presente en entorno\n",
            "        OPENAI_API_KEY no configurada ‚Äî correcto\n",
            "  ‚úÖ  0.10: Anthropic como provider opcional\n",
            "        7/7 checks pasaron\n",
            "  ‚úÖ  0.2: Google AI SDK instalado y funcional\n",
            "        3/3 checks pasaron\n",
            "  ‚úÖ  0.3: EmbeddingService migrado a Google\n",
            "        6/6 checks pasaron\n",
            "  ‚úÖ  0.3.1: Generar embedding real con Google API\n",
            "        dim=3072, float=True, non_zero=True\n",
            "  ‚úÖ  0.3.2: Agregar documento + b√∫squeda sem√°ntica ChromaDB\n",
            "        add=True, search_results=1\n",
            "  ‚úÖ  0.4: DocumentProcessor usa embeddings Google\n",
            "        5/5 checks pasaron\n",
            "  ‚úÖ  0.5: WatcherService migrado a Gemini\n",
            "        5/5 checks pasaron\n",
            "  ‚è≠Ô∏è  0.5.1: An√°lisis de fragmento con Gemini\n",
            "        Rate limit alcanzado (quota free tier agotada). El c√≥digo es correcto, reintentar ma√±ana.\n",
            "  ‚úÖ  0.6: InsightReportingAgent migrado a Gemini\n",
            "        5/5 checks pasaron\n",
            "  ‚úÖ  0.7: AgentSystemConfig actualizado\n",
            "        6/6 checks pasaron\n",
            "  ‚úÖ  0.8: LangChain migrado a langchain-google-genai\n",
            "        6/6 checks pasaron\n",
            "  ‚úÖ  0.9: ChromaDB reindexaci√≥n con Google embeddings\n",
            "        3/3 checks pasaron\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "  Total: 13 tests | Pasaron: 13 | Skipped (rate limit): 1\n",
            "  Tasa de √©xito: 100.0%\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "  ‚úÖ √âPICA 0 COMPLETADA ‚Äî 1 test(s) skipped por rate limit (c√≥digo correcto)\n"
          ]
        }
      ],
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# RESUMEN FINAL DE √âPICA 0\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"  RESUMEN DE RESULTADOS ‚Äî √âPICA 0: MIGRACI√ìN OPENAI ‚Üí GOOGLE GEMINI\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "total = len(RESULTS)\n",
        "passed = sum(1 for r in RESULTS.values() if r[\"passed\"])\n",
        "skipped = sum(1 for r in RESULTS.values() if r.get(\"skipped\", False))\n",
        "failed = total - passed\n",
        "\n",
        "for ticket, result in sorted(RESULTS.items()):\n",
        "    if result.get(\"skipped\"):\n",
        "        status = \"‚è≠Ô∏è\"\n",
        "    elif result[\"passed\"]:\n",
        "        status = \"‚úÖ\"\n",
        "    else:\n",
        "        status = \"‚ùå\"\n",
        "    print(f\"  {status}  {ticket}: {result['name']}\")\n",
        "    if result[\"details\"]:\n",
        "        print(f\"        {result['details']}\")\n",
        "\n",
        "print()\n",
        "print(\"-\" * 70)\n",
        "summary_parts = [f\"Total: {total} tests\", f\"Pasaron: {passed}\"]\n",
        "if skipped > 0:\n",
        "    summary_parts.append(f\"Skipped (rate limit): {skipped}\")\n",
        "if failed > 0:\n",
        "    summary_parts.append(f\"Fallaron: {failed}\")\n",
        "print(f\"  {' | '.join(summary_parts)}\")\n",
        "print(f\"  Tasa de √©xito: {passed/total*100:.1f}%\" if total > 0 else \"  Sin tests\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "if failed == 0 and skipped == 0:\n",
        "    print(\"\\n  üéâ √âPICA 0 COMPLETADA ‚Äî Todos los tests pasaron\")\n",
        "elif failed == 0 and skipped > 0:\n",
        "    print(f\"\\n  ‚úÖ √âPICA 0 COMPLETADA ‚Äî {skipped} test(s) skipped por rate limit (c√≥digo correcto)\")\n",
        "else:\n",
        "    print(f\"\\n  ‚ö†Ô∏è  {failed} test(s) fallaron ‚Äî revisar detalles arriba\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Notas y observaciones\n",
        "\n",
        "### Decisiones de arquitectura\n",
        "- **Modelo de embeddings:** `gemini-embedding-001` (3072 dims) reemplaza `text-embedding-3-small` (1536 dims)\n",
        "- **Modelo de chat:** `gemini-2.0-flash` reemplaza `gpt-3.5-turbo`\n",
        "- **Provider alternativo:** Anthropic `claude-3-5-sonnet` disponible via `LLMProviderFactory`\n",
        "- **ChromaDB:** Usa `GoogleEmbeddingFunction` como embedding function nativa\n",
        "\n",
        "### Hallazgos durante testing\n",
        "- `gemini-2.0-flash-exp` fue deprecado por Google ‚Üí actualizado a `gemini-2.0-flash` (GA)\n",
        "- `google.generativeai` package est√° deprecado ‚Üí migrar a `google.genai` (√âpica 7 - Deuda T√©cnica)\n",
        "- `DocumentProcessor` usa `tiktoken` (OpenAI tokenizer) para chunking ‚Üí considerar migrar a tokenizer gen√©rico\n",
        "- Python 3.9 est√° EOL, Google muestra FutureWarnings ‚Üí planificar upgrade a 3.11+\n",
        "\n",
        "### Consideraciones\n",
        "- Los embeddings de Google tienen mayor dimensionalidad (3072 vs 1536), lo que implica m√°s almacenamiento pero potencialmente mejor precisi√≥n\n",
        "- La re-indexaci√≥n completa de ChromaDB es necesaria al cambiar de modelo de embeddings\n",
        "- `gemini-2.0-flash` tiene l√≠mites generosos de rate limiting (1M tokens/min)\n",
        "\n",
        "### Pr√≥ximos pasos\n",
        "- √âpica 1: Ingesta de boletines\n",
        "- √âpica 2: Extracci√≥n de entidades\n",
        "- √âpica 3: Feature Engineering"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
