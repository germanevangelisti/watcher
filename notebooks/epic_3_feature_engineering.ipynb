{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epic 3: Feature Engineering - Validation Notebook\n",
    "\n",
    "Este notebook valida las 3 tareas completadas:\n",
    "\n",
    "- **3.3**: Text Cleaning (ftfy + normalización)\n",
    "- **3.1**: Chunking Strategy (recursivo con separadores jerárquicos)\n",
    "- **3.2**: ChunkRecord (metadata enriquecida)\n",
    "\n",
    "Usaremos boletines reales para demostrar cada funcionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../watcher-monolith/backend')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set environment\n",
    "os.environ['PYTHONPATH'] = str(Path.cwd().parent / 'watcher-monolith' / 'backend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Validación de TextCleaner (Tarea 3.3)\n",
    "\n",
    "Probamos la limpieza de texto con ejemplos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy not installed. Install with: pip install ftfy\n",
      "ftfy no disponible, fix_encoding será ignorado\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEXTO ORIGINAL ===\n",
      "\n",
      "Página 1 de 50\n",
      "\n",
      "DECRETO  N°   12345\n",
      "\n",
      "Art.  5º -    Apruébase   la   designación     de   Juan   Pérez.\n",
      "\n",
      "El   monto   es   de   $   150.000   pesos.\n",
      "\n",
      "________________\n",
      "\n",
      "Página 2 de 50\n",
      "\n",
      "Art. 6º - Continúa...\n",
      "\n",
      "\n",
      "Longitud: 206 caracteres\n",
      "\n",
      "=== TEXTO LIMPIO ===\n",
      "DECRETO 12345\n",
      "\n",
      "ARTICULO 5o - Apruébase la designación de Juan Pérez.\n",
      "\n",
      "El monto es de pesos 150.000 pesos.\n",
      "\n",
      "ARTICULO 6o - Continúa...\n",
      "\n",
      "Longitud: 132 caracteres\n",
      "\n",
      "=== MEJORAS ===\n",
      "- Removidos números de página\n",
      "- Removidas líneas de separadores\n",
      "- Espacios múltiples colapsados\n",
      "- Abreviaturas normalizadas (Art. -> ARTICULO)\n",
      "- Símbolo $ normalizado\n"
     ]
    }
   ],
   "source": [
    "from app.services.text_cleaner import TextCleaner, CleaningConfig\n",
    "\n",
    "# Crear instancia\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "# Texto de ejemplo con problemas típicos de PDFs\n",
    "dirty_text = \"\"\"\n",
    "Página 1 de 50\n",
    "\n",
    "DECRETO  N°   12345\n",
    "\n",
    "Art.  5º -    Apruébase   la   designación     de   Juan   Pérez.\n",
    "\n",
    "El   monto   es   de   $   150.000   pesos.\n",
    "\n",
    "________________\n",
    "\n",
    "Página 2 de 50\n",
    "\n",
    "Art. 6º - Continúa...\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== TEXTO ORIGINAL ===\")\n",
    "print(dirty_text)\n",
    "print(f\"\\nLongitud: {len(dirty_text)} caracteres\\n\")\n",
    "\n",
    "# Limpiar\n",
    "cleaned_text = cleaner.clean(dirty_text)\n",
    "\n",
    "print(\"=== TEXTO LIMPIO ===\")\n",
    "print(cleaned_text)\n",
    "print(f\"\\nLongitud: {len(cleaned_text)} caracteres\")\n",
    "\n",
    "print(\"\\n=== MEJORAS ===\")\n",
    "print(\"- Removidos números de página\")\n",
    "print(\"- Removidas líneas de separadores\")\n",
    "print(\"- Espacios múltiples colapsados\")\n",
    "print(\"- Abreviaturas normalizadas (Art. -> ARTICULO)\")\n",
    "print(\"- Símbolo $ normalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar cada paso de limpieza individualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mojibake: CÃ³rdoba - AÃ±o 2025\n",
      "Fixed: CÃ³rdoba - AÃ±o 2025\n",
      "\n",
      "Original: 'niño'\n",
      "Normalized: 'niño'\n",
      "\n",
      "Whitespace original: 'Hola    mundo.\\n\\n\\n\\n\\nNueva   línea.'\n",
      "Whitespace normalizado: 'Hola mundo.\\n\\nNueva línea.'\n"
     ]
    }
   ],
   "source": [
    "# Probar encoding fix con texto mojibake\n",
    "mojibake_text = \"CÃ³rdoba - AÃ±o 2025\"\n",
    "fixed = cleaner.fix_encoding(mojibake_text)\n",
    "print(f\"Mojibake: {mojibake_text}\")\n",
    "print(f\"Fixed: {fixed}\")\n",
    "\n",
    "# Probar normalización unicode\n",
    "unicode_text = \"niño\"  # Usando composición\n",
    "normalized = cleaner.normalize_unicode(unicode_text)\n",
    "print(f\"\\nOriginal: {repr(unicode_text)}\")\n",
    "print(f\"Normalized: {repr(normalized)}\")\n",
    "\n",
    "# Probar normalización de whitespace\n",
    "ws_text = \"Hola    mundo.\\n\\n\\n\\n\\nNueva   línea.\"\n",
    "normalized_ws = cleaner.normalize_whitespace(ws_text)\n",
    "print(f\"\\nWhitespace original: {repr(ws_text)}\")\n",
    "print(f\"Whitespace normalizado: {repr(normalized_ws)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validación de ChunkingService (Tarea 3.1)\n",
    "\n",
    "Probamos la estrategia recursiva de chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHUNKING RESULTS ===\n",
      "Texto original: 378 caracteres\n",
      "Total chunks: 4\n",
      "\n",
      "--- Chunk 0 ---\n",
      "Índice: 0\n",
      "Posición: 0-131\n",
      "Tamaño: 131 chars\n",
      "Texto: \n",
      "DECRETO 123\n",
      "\n",
      "ARTICULO 1 - Se aprueba el presupuesto anual 2025 de la Provincia de Córdoba por un mo...\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Índice: 1\n",
      "Posición: 81-278\n",
      "Tamaño: 197 chars\n",
      "Texto: 1 - Se aprueba el presupuesto anual 2025 de la Provincia de Córdoba por un monto total de pesos 50.0...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Índice: 2\n",
      "Posición: 228-384\n",
      "Tamaño: 156 chars\n",
      "Texto: 2 - Los organismos deberán ajustarse a las partidas asignadas.\n",
      "\n",
      "RESOLUCION 456\n",
      "\n",
      "ARTICULO 1 - Se desi...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Índice: 3\n",
      "Posición: 334-482\n",
      "Tamaño: 148 chars\n",
      "Texto: 1 - Se designa a María García como Directora del Área de Finanzas.\n",
      "\n",
      "ARTICULO 2 - La presente resoluc...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.services.chunking_service import ChunkingService, ChunkingConfig\n",
    "\n",
    "# Crear servicio\n",
    "chunker = ChunkingService()\n",
    "\n",
    "# Texto de ejemplo con estructura de boletín\n",
    "boletin_text = \"\"\"\n",
    "DECRETO 123\n",
    "\n",
    "ARTICULO 1 - Se aprueba el presupuesto anual 2025 de la Provincia de Córdoba por un monto total de pesos 50.000.000.\n",
    "\n",
    "ARTICULO 2 - Los organismos deberán ajustarse a las partidas asignadas.\n",
    "\n",
    "RESOLUCION 456\n",
    "\n",
    "ARTICULO 1 - Se designa a María García como Directora del Área de Finanzas.\n",
    "\n",
    "ARTICULO 2 - La presente resolución tendrá vigencia a partir de su publicación.\n",
    "\"\"\"\n",
    "\n",
    "# Configurar chunking pequeño para demostración\n",
    "config = ChunkingConfig(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=50,\n",
    "    min_chunk_size=50\n",
    ")\n",
    "\n",
    "# Chunking\n",
    "chunks = chunker.chunk(boletin_text, config)\n",
    "\n",
    "print(f\"=== CHUNKING RESULTS ===\")\n",
    "print(f\"Texto original: {len(boletin_text)} caracteres\")\n",
    "print(f\"Total chunks: {len(chunks)}\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(f\"Índice: {chunk.chunk_index}\")\n",
    "    print(f\"Posición: {chunk.start_char}-{chunk.end_char}\")\n",
    "    print(f\"Tamaño: {chunk.num_chars} chars\")\n",
    "    print(f\"Texto: {chunk.text[:100]}...\" if len(chunk.text) > 100 else f\"Texto: {chunk.text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparar chunking viejo vs nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/germanevangelisti/watcher-agent/.venv/lib/python3.9/site-packages/google/api_core/_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.10). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/germanevangelisti/watcher-agent/.venv/lib/python3.9/site-packages/google/auth/__init__.py:54: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/germanevangelisti/watcher-agent/.venv/lib/python3.9/site-packages/google/oauth2/__init__.py:40: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/germanevangelisti/watcher-agent/notebooks/../watcher-monolith/backend/app/services/embedding_service.py:48: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n",
      "ftfy no disponible, fix_encoding será ignorado\n",
      "Google API key not found. Falling back to local embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto de prueba: 1400 caracteres\n",
      "\n",
      "=== NUEVO CHUNKING (ChunkingService) ===\n",
      "Total chunks: 2\n",
      "Tamaños: [1062, 362]\n",
      "\n",
      "=== LEGACY CHUNKING ===\n",
      "Total chunks: 2\n",
      "Tamaños: [1062, 362]\n",
      "\n",
      "✅ Ambos producen resultados similares, pero el nuevo usa separadores inteligentes\n"
     ]
    }
   ],
   "source": [
    "from app.services.embedding_service import EmbeddingService\n",
    "\n",
    "# Texto de prueba\n",
    "test_text = \"Lorem ipsum dolor sit amet. \" * 50  # ~1400 chars\n",
    "\n",
    "print(f\"Texto de prueba: {len(test_text)} caracteres\\n\")\n",
    "\n",
    "# Nuevo chunking (con ChunkingService)\n",
    "new_chunks = chunker.chunk(test_text)\n",
    "print(f\"=== NUEVO CHUNKING (ChunkingService) ===\")\n",
    "print(f\"Total chunks: {len(new_chunks)}\")\n",
    "print(f\"Tamaños: {[c.num_chars for c in new_chunks]}\")\n",
    "\n",
    "# Legacy chunking\n",
    "service = EmbeddingService()\n",
    "legacy_chunks = service.chunk_text(test_text)\n",
    "print(f\"\\n=== LEGACY CHUNKING ===\")\n",
    "print(f\"Total chunks: {len(legacy_chunks)}\")\n",
    "print(f\"Tamaños: {[len(c) for c in legacy_chunks]}\")\n",
    "\n",
    "print(\"\\n✅ Ambos producen resultados similares, pero el nuevo usa separadores inteligentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validación de ChunkEnricher (Tarea 3.2)\n",
    "\n",
    "Probamos el enriquecimiento de chunks con metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHUNK ENRICHMENT ===\n",
      "\n",
      "--- Chunk 0 ---\n",
      "Texto: DECRETO 123. ARTICULO 1 - Se aprueba el presupuesto por pesos 50.000.0...\n",
      "Esperado: decreto, has_amounts=True\n",
      "\n",
      "Detectado:\n",
      "  - section_type: decreto\n",
      "  - has_amounts: True\n",
      "  - has_tables: False\n",
      "  - entities: {'montos': ['pesos 50']}\n",
      "  - chunk_hash: c14d0978e81d7fc0...\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Texto: LICITACION PUBLICA para la provisión de 100 computadoras por $2.000.00...\n",
      "Esperado: licitacion, has_amounts=True\n",
      "\n",
      "Detectado:\n",
      "  - section_type: licitacion\n",
      "  - has_amounts: True\n",
      "  - has_tables: False\n",
      "  - entities: {'montos': ['$2', '$2.000']}\n",
      "  - chunk_hash: 47cacfc7df0e6e54...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Texto: Se designa a Juan Pérez como Director del Área de Recursos Humanos.\n",
      "Esperado: nombramiento, entidades: personas\n",
      "\n",
      "Detectado:\n",
      "  - section_type: nombramiento\n",
      "  - has_amounts: False\n",
      "  - has_tables: False\n",
      "  - entities: {'organismos': ['Se', 'Director'], 'personas': ['Juan Pérez', 'Recursos Humanos']}\n",
      "  - chunk_hash: ba70df4c8f6c89ef...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Texto: Presupuesto Anual\n",
      "Programa | Monto\n",
      "Salud | 1000000\n",
      "Educación | 2000000\n",
      "Esperado: presupuesto, has_tables=True\n",
      "\n",
      "Detectado:\n",
      "  - section_type: presupuesto\n",
      "  - has_amounts: False\n",
      "  - has_tables: False\n",
      "  - entities: {'personas': ['Presupuesto Anual', 'Monto\\nSalud']}\n",
      "  - chunk_hash: 80130fc8ee5e1385...\n"
     ]
    }
   ],
   "source": [
    "from app.services.chunk_enricher import ChunkEnricher\n",
    "\n",
    "# Crear enricher\n",
    "enricher = ChunkEnricher()\n",
    "\n",
    "# Chunks de ejemplo con diferentes tipos de contenido\n",
    "test_chunks = [\n",
    "    {\n",
    "        \"text\": \"DECRETO 123. ARTICULO 1 - Se aprueba el presupuesto por pesos 50.000.000.\",\n",
    "        \"expected\": \"decreto, has_amounts=True\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"LICITACION PUBLICA para la provisión de 100 computadoras por $2.000.000.\",\n",
    "        \"expected\": \"licitacion, has_amounts=True\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Se designa a Juan Pérez como Director del Área de Recursos Humanos.\",\n",
    "        \"expected\": \"nombramiento, entidades: personas\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Presupuesto Anual\\nPrograma | Monto\\nSalud | 1000000\\nEducación | 2000000\",\n",
    "        \"expected\": \"presupuesto, has_tables=True\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=== CHUNK ENRICHMENT ===\")\n",
    "\n",
    "for i, test in enumerate(test_chunks):\n",
    "    enriched = enricher.enrich(\n",
    "        chunk_text=test[\"text\"],\n",
    "        chunk_index=i,\n",
    "        document_id=\"test_doc\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"Texto: {test['text'][:70]}...\" if len(test['text']) > 70 else f\"Texto: {test['text']}\")\n",
    "    print(f\"Esperado: {test['expected']}\")\n",
    "    print(f\"\\nDetectado:\")\n",
    "    print(f\"  - section_type: {enriched['section_type']}\")\n",
    "    print(f\"  - has_amounts: {enriched['has_amounts']}\")\n",
    "    print(f\"  - has_tables: {enriched['has_tables']}\")\n",
    "    print(f\"  - entities: {enriched['entities_json']}\")\n",
    "    print(f\"  - chunk_hash: {enriched['chunk_hash'][:16]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline completo integrado\n",
    "\n",
    "Demostramos el pipeline completo: Clean → Chunk → Enrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PIPELINE COMPLETO ===\n",
      "\n",
      "1. TEXTO ORIGINAL (266 chars)\n",
      "\n",
      "Página 1\n",
      "\n",
      "DECRETO  N°  12345\n",
      "\n",
      "Art.  1º  -  Se  aprueba  el  presupuesto  por  $  10.000.000.\n",
      "\n",
      "_____\n",
      "\n",
      "2. DESPUÉS DE LIMPIEZA (212 chars)\n",
      "DECRETO 12345\n",
      "\n",
      "ARTICULO 1o - Se aprueba el presupuesto por pesos 10.000.000.\n",
      "\n",
      "ARTICULO 2o - Se desig\n",
      "\n",
      "3. DESPUÉS DE CHUNKING: 2 chunks\n",
      "\n",
      "4. CHUNKS ENRIQUECIDOS:\n",
      "\n",
      "  Chunk 0:\n",
      "    - Texto: DECRETO 12345\n",
      "\n",
      "ARTICULO 1o - Se aprueba el presupu...\n",
      "    - Tipo: decreto\n",
      "    - Montos: True\n",
      "    - Entidades: ['montos', 'organismos', 'personas']\n",
      "\n",
      "  Chunk 1:\n",
      "    - Texto: 2o - Se designa a María García como Directora.\n",
      "\n",
      "RE...\n",
      "    - Tipo: licitacion\n",
      "    - Montos: False\n",
      "    - Entidades: ['organismos', 'personas']\n"
     ]
    }
   ],
   "source": [
    "# Texto sucio de boletin\n",
    "raw_boletin = \"\"\"\n",
    "Página 1\n",
    "\n",
    "DECRETO  N°  12345\n",
    "\n",
    "Art.  1º  -  Se  aprueba  el  presupuesto  por  $  10.000.000.\n",
    "\n",
    "______________\n",
    "\n",
    "Página 2\n",
    "\n",
    "Art.  2º  -  Se  designa  a  María  García  como  Directora.\n",
    "\n",
    "RESOLUCION  N°  456\n",
    "\n",
    "Art.  1º  -  Licitación  para  adquisición  de  equipamiento.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== PIPELINE COMPLETO ===\")\n",
    "print(f\"\\n1. TEXTO ORIGINAL ({len(raw_boletin)} chars)\")\n",
    "print(raw_boletin[:100])\n",
    "\n",
    "# Paso 1: Limpiar\n",
    "cleaned = cleaner.clean(raw_boletin)\n",
    "print(f\"\\n2. DESPUÉS DE LIMPIEZA ({len(cleaned)} chars)\")\n",
    "print(cleaned[:100])\n",
    "\n",
    "# Paso 2: Chunkear\n",
    "config = ChunkingConfig(chunk_size=150, chunk_overlap=30)\n",
    "chunks = chunker.chunk(cleaned, config)\n",
    "print(f\"\\n3. DESPUÉS DE CHUNKING: {len(chunks)} chunks\")\n",
    "\n",
    "# Paso 3: Enriquecer\n",
    "enriched_chunks = []\n",
    "for chunk_result in chunks:\n",
    "    enriched = enricher.enrich(\n",
    "        chunk_text=chunk_result.text,\n",
    "        chunk_index=chunk_result.chunk_index,\n",
    "        document_id=\"boletin_12345\",\n",
    "        context={\n",
    "            \"start_char\": chunk_result.start_char,\n",
    "            \"end_char\": chunk_result.end_char\n",
    "        }\n",
    "    )\n",
    "    enriched_chunks.append(enriched)\n",
    "\n",
    "print(f\"\\n4. CHUNKS ENRIQUECIDOS:\")\n",
    "for i, ec in enumerate(enriched_chunks):\n",
    "    print(f\"\\n  Chunk {i}:\")\n",
    "    print(f\"    - Texto: {ec['text'][:50]}...\")\n",
    "    print(f\"    - Tipo: {ec['section_type']}\")\n",
    "    print(f\"    - Montos: {ec['has_amounts']}\")\n",
    "    print(f\"    - Entidades: {list(ec['entities_json'].keys()) if ec['entities_json'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validación con boletín real (opcional)\n",
    "\n",
    "Si hay PDFs disponibles en data/raw, procesarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 99 PDFs\n",
      "Ejemplo: 20250828_3_Secc.pdf\n",
      "\n",
      "Procesando: ../watcher-monolith/backend/data/raw/20250828_3_Secc.pdf\n",
      "✅ Extracción exitosa\n",
      "   Páginas: 34\n",
      "   Caracteres: 265810\n",
      "\n",
      "✅ Limpieza completa\n",
      "   Caracteres después: 266849\n",
      "\n",
      "✅ Chunking completo\n",
      "   Total chunks: 347\n",
      "   Tamaño promedio: 889 chars\n",
      "\n",
      "  Chunk 0:\n",
      "    - Texto: a\n",
      "SOCIEDADES - PERSONAS\n",
      "JUEVES 28 DE AGOSTO DE 2025 JURÍDICAS - ASAMBLEAS Y OTRAS\n",
      "AÑO CXII - TOMO DC...\n",
      "    - Tipo: nombramiento\n",
      "    - Montos: False\n",
      "    - Tablas: False\n",
      "\n",
      "  Chunk 1:\n",
      "    - Texto: VILLA MARÍA por las cuales esta asamblea fue realizada fuera de la Asamblea de fecha 19/10/2019 3) R...\n",
      "    - Tipo: nombramiento\n",
      "    - Montos: False\n",
      "    - Tablas: False\n",
      "\n",
      "  Chunk 2:\n",
      "    - Texto: BLEA ANUAL ORDINARIA, a realizarse el día 05 y Comisión de Fiscalización, para los cargos que nes po...\n",
      "    - Tipo: nombramiento\n",
      "    - Montos: False\n",
      "    - Tablas: False\n",
      "\n",
      "  Chunk 3:\n",
      "    - Texto: junto con el presidente y secretario. 2) Motivos de FRANCISCO En la localidad de Tránsito, Dpto. San...\n",
      "    - Tipo: general\n",
      "    - Montos: False\n",
      "    - Tablas: False\n",
      "\n",
      "  Chunk 4:\n",
      "    - Texto: Fiscalizadora e Informe del Auditor, correspon- brarse el día 18 de Septiembre de 2025, a las 19.30 ...\n",
      "    - Tipo: nombramiento\n",
      "    - Montos: False\n",
      "    - Tablas: False\n",
      "\n",
      "  Chunk 5:\n",
      "    - Texto: miembros titulares y un miembro suplente, por el Comisión Directiva. La Secretaria.- DNI: 29.016.726...\n",
      "    - Tipo: general\n",
      "    - Montos: False\n",
      "    - Tablas: False\n",
      "\n",
      "  Chunk 6:\n",
      "    - Texto: Gustavo Ricchiardi y Daniel Aronne, respectiva- Convocatoria a Asamblea General Ordinaria Por la pal...\n",
      "    - Tipo: general\n",
      "    - Montos: False\n",
      "    - Tablas: False\n",
      "\n",
      "  Chunk 7:\n",
      "    - Texto: Convocatoria a Asamblea Por Acta de Comisión social sita en Calle Lisandro De La Torre 33, ciu- rida...\n",
      "    - Tipo: nombramiento\n",
      "    - Montos: False\n",
      "    - Tablas: False\n",
      "\n",
      "  Chunk 8:\n",
      "    - Texto: 3a\n",
      "SOCIEDADES - PERSONAS JURÍDICAS AÑO CXII - TOMO DCCXXVIII - No 166\n",
      "CORDOBA, (R.A.), JUEVES 28 DE ...\n",
      "    - Tipo: nombramiento\n",
      "    - Montos: False\n",
      "    - Tablas: False\n",
      "\n",
      "  Chunk 9:\n",
      "    - Texto: tratar el siguiente orden del día: 1) Designación de disposición en la sede social. ret No3182, Piso...\n",
      "    - Tipo: nombramiento\n",
      "    - Montos: True\n",
      "    - Tablas: False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Buscar un PDF de ejemplo\n",
    "data_dir = Path('../watcher-monolith/backend/data/raw')\n",
    "\n",
    "if data_dir.exists():\n",
    "    pdf_files = list(data_dir.glob('*.pdf'))\n",
    "    if pdf_files:\n",
    "        print(f\"Encontrados {len(pdf_files)} PDFs\")\n",
    "        print(f\"Ejemplo: {pdf_files[0].name}\")\n",
    "        \n",
    "        # Extraer y procesar\n",
    "        from app.services.extractors.registry import ExtractorRegistry\n",
    "        \n",
    "        registry = ExtractorRegistry()\n",
    "        \n",
    "        # Procesar solo el primero\n",
    "        pdf_path = pdf_files[0]  # Ya es un Path object\n",
    "        print(f\"\\nProcesando: {pdf_path}\")\n",
    "        \n",
    "        # Extraer (async - necesita await)\n",
    "        result = await registry.extract(pdf_path)\n",
    "        \n",
    "        if result.success:\n",
    "            print(f\"✅ Extracción exitosa\")\n",
    "            print(f\"   Páginas: {len(result.pages)}\")\n",
    "            print(f\"   Caracteres: {len(result.full_text)}\")\n",
    "            \n",
    "            # Limpiar\n",
    "            cleaned = cleaner.clean(result.full_text)\n",
    "            print(f\"\\n✅ Limpieza completa\")\n",
    "            print(f\"   Caracteres después: {len(cleaned)}\")\n",
    "            \n",
    "            # Chunkear\n",
    "            chunks = chunker.chunk(cleaned)\n",
    "            print(f\"\\n✅ Chunking completo\")\n",
    "            print(f\"   Total chunks: {len(chunks)}\")\n",
    "            print(f\"   Tamaño promedio: {sum(c.num_chars for c in chunks) / len(chunks):.0f} chars\")\n",
    "            \n",
    "            # Enriquecer muestra\n",
    "            sample_chunks = chunks[:10]  # Solo primeros 3\n",
    "            for chunk_result in sample_chunks:\n",
    "                enriched = enricher.enrich(\n",
    "                    chunk_text=chunk_result.text,\n",
    "                    chunk_index=chunk_result.chunk_index,\n",
    "                    document_id=pdf_files[0].stem\n",
    "                )\n",
    "                print(f\"\\n  Chunk {enriched['chunk_index']}:\")\n",
    "                print(f\"    - Texto: {chunk_result.text[:100]}...\")\n",
    "                print(f\"    - Tipo: {enriched['section_type']}\")\n",
    "                print(f\"    - Montos: {enriched['has_amounts']}\")\n",
    "                print(f\"    - Tablas: {enriched['has_tables']}\")\n",
    "        else:\n",
    "            print(f\"❌ Error: {result.error}\")\n",
    "    else:\n",
    "        print(\"No se encontraron PDFs en data/raw\")\n",
    "else:\n",
    "    print(f\"Directorio no existe: {data_dir}\")\n",
    "    print(\"Saltando validación con PDF real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "✅ **Tarea 3.3** (TextCleaner): Validado con ejemplos de limpieza\n",
    "\n",
    "✅ **Tarea 3.1** (ChunkingService): Validado con estrategia recursiva\n",
    "\n",
    "✅ **Tarea 3.2** (ChunkEnricher): Validado con enriquecimiento de metadata\n",
    "\n",
    "✅ **Pipeline integrado**: Clean → Chunk → Enrich funcionando correctamente\n",
    "\n",
    "### Próximos pasos\n",
    "\n",
    "1. Ejecutar migración Alembic para crear tabla `chunk_records`\n",
    "2. Probar persistencia de ChunkRecords con EmbeddingService\n",
    "3. Implementar Epic 4 (Indexación triple: vector + relacional + full-text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
