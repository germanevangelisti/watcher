{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Epic 6: Sistema Ag√©ntico - Validaci√≥n End-to-End\n",
        "\n",
        "Este notebook valida las 3 tareas implementadas en la √âpica 6:\n",
        "\n",
        "- **6.3**: LLMProviderFactory con soporte multi-provider\n",
        "- **6.1**: Generaci√≥n real de respuestas en RAGAgent usando LLM\n",
        "- **6.2**: Integraci√≥n de agentes con pipeline de retrieval h√≠brido\n",
        "\n",
        "## ‚ö†Ô∏è Nota: Python 3.9 y Deprecation Warnings\n",
        "\n",
        "El SDK `google-generativeai` est√° deprecado pero funcional hasta Junio 2026.\n",
        "\n",
        "Ver√°s warnings de deprecaci√≥n al ejecutar - esto es normal y **no afecta la funcionalidad**.\n",
        "\n",
        "Para eliminar warnings: actualizar a Python 3.10+ (ver `PYTHON_39_LIMITATION.md`).\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Backend path: /Users/germanevangelisti/watcher-agent/watcher-monolith/backend\n",
            "‚úÖ Parent path: /Users/germanevangelisti/watcher-agent\n",
            "‚úÖ Root agents path: /Users/germanevangelisti/watcher-agent/agents\n",
            "‚úÖ Python version: 3.9.10 (main, Oct 11 2024, 16:02:49) \n",
            "[Clang 15.0.0 (clang-1500.3.9.4)]\n",
            "‚úÖ Warnings suppressed (Python 3.9 deprecation notices)\n",
            "‚úÖ RAGAgent imported from root agents/\n",
            "\n",
            "üìç sys.path order (first 3):\n",
            "   0: /Users/germanevangelisti/watcher-agent/watcher-monolith/backend\n",
            "   1: /Users/germanevangelisti/watcher-agent/watcher-monolith/backend\n",
            "   2: /Users/germanevangelisti/watcher-agent\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import asyncio\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Suppress Python 3.9 deprecation warnings (they're expected, see PYTHON_39_LIMITATION.md)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Set up paths for importing from different locations\n",
        "backend_path = Path.cwd().parent / \"watcher-monolith\" / \"backend\"\n",
        "parent_path = Path.cwd().parent\n",
        "root_agents_path = parent_path / \"agents\"\n",
        "\n",
        "# Insert paths in order:\n",
        "# 1. Backend (for backend agents like InsightReportingAgent, orchestrator, tools)\n",
        "# 2. Parent (for general imports)\n",
        "sys.path.insert(0, str(parent_path))\n",
        "sys.path.insert(0, str(backend_path))\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "env_path = backend_path / \".env\"\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# Import root-level agents by temporarily prioritizing parent path\n",
        "# This allows us to import RAGAgent from root agents/\n",
        "import importlib.util\n",
        "\n",
        "def import_root_agent(agent_name, file_name):\n",
        "    \"\"\"Helper to import agents from root-level agents/ directory\"\"\"\n",
        "    agent_path = root_agents_path / file_name\n",
        "    spec = importlib.util.spec_from_file_location(agent_name, agent_path)\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(module)\n",
        "    return module\n",
        "\n",
        "# Pre-import RAGAgent from root\n",
        "raga_module = import_root_agent(\"agents.raga_agent\", \"raga_agent.py\")\n",
        "RAGAgent = raga_module.RAGAgent\n",
        "\n",
        "# Make RAGAgent available globally for subsequent cells\n",
        "globals()['RAGAgent'] = RAGAgent\n",
        "\n",
        "print(f\"‚úÖ Backend path: {backend_path}\")\n",
        "print(f\"‚úÖ Parent path: {parent_path}\")\n",
        "print(f\"‚úÖ Root agents path: {root_agents_path}\")\n",
        "print(f\"‚úÖ Python version: {sys.version}\")\n",
        "print(f\"‚úÖ Warnings suppressed (Python 3.9 deprecation notices)\")\n",
        "print(f\"‚úÖ RAGAgent imported from root agents/\")\n",
        "print(f\"\\nüìç sys.path order (first 3):\")\n",
        "for i, p in enumerate(sys.path[:3]):\n",
        "    print(f\"   {i}: {p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test-6.3",
      "metadata": {},
      "source": [
        "## Test 6.3: LLMProviderFactory\n",
        "\n",
        "Verificar que la factory puede crear providers desde configuraci√≥n de entorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "test-llm-factory",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Test 6.3: LLMProviderFactory\n",
            "============================================================\n",
            "\n",
            "1. Creating provider from environment...\n",
            "‚úÖ Provider created: GoogleGeminiProvider\n",
            "   Model: gemini-2.0-flash\n",
            "\n",
            "2. Verifying environment configuration...\n",
            "   LLM_PROVIDER: google\n",
            "   LLM_MODEL: gemini-2.0-flash\n",
            "‚úÖ Environment variables configured\n",
            "\n",
            "3. Testing text generation...\n",
            "‚úÖ Text generation works\n",
            "   Response: Buenos Aires\n",
            "\n",
            "4. Testing text generation with system prompt...\n",
            "‚úÖ System prompt generation works\n",
            "   Response: El Decreto 123/2025 formaliza el nombramiento de Juan P√©rez como Director, otorg√°ndole las facultades y responsabilidades inherentes al cargo dentro de la administraci√≥n p√∫blica. Su validez depende del cumplimiento de los requisitos legales y administrativos para la designaci√≥n.\n",
            "\n",
            "============================================================\n",
            "‚úÖ Test 6.3 completado exitosamente\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from app.services.llm_provider import get_llm_provider, LLMProviderType, LLMProviderFactory\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Test 6.3: LLMProviderFactory\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test 1: Create provider from environment\n",
        "print(\"\\n1. Creating provider from environment...\")\n",
        "provider = get_llm_provider()\n",
        "print(f\"‚úÖ Provider created: {provider.__class__.__name__}\")\n",
        "print(f\"   Model: {provider.model_name}\")\n",
        "\n",
        "# Test 2: Verify environment configuration\n",
        "print(\"\\n2. Verifying environment configuration...\")\n",
        "llm_provider = os.getenv(\"LLM_PROVIDER\", \"not set\")\n",
        "llm_model = os.getenv(\"LLM_MODEL\", \"not set\")\n",
        "print(f\"   LLM_PROVIDER: {llm_provider}\")\n",
        "print(f\"   LLM_MODEL: {llm_model}\")\n",
        "print(f\"‚úÖ Environment variables configured\")\n",
        "\n",
        "# Test 3: Generate simple text\n",
        "print(\"\\n3. Testing text generation...\")\n",
        "response = await provider.generate_text(\n",
        "    prompt=\"¬øCu√°l es la capital de Argentina? Responde en una sola palabra.\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=50\n",
        ")\n",
        "print(f\"‚úÖ Text generation works\")\n",
        "print(f\"   Response: {response}\")\n",
        "\n",
        "# Test 4: Generate with system prompt\n",
        "print(\"\\n4. Testing text generation with system prompt...\")\n",
        "response = await provider.generate_text(\n",
        "    prompt=\"Analiza el siguiente decreto: 'DECRETO 123/2025 - Se designa a Juan P√©rez como Director'\",\n",
        "    system_prompt=\"Eres un experto en an√°lisis de decretos argentinos. Responde de forma concisa en m√°ximo 2 oraciones.\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=100\n",
        ")\n",
        "print(f\"‚úÖ System prompt generation works\")\n",
        "print(f\"   Response: {response}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Test 6.3 completado exitosamente\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test-6.1",
      "metadata": {},
      "source": [
        "## Test 6.1: RAGAgent con Generaci√≥n Real\n",
        "\n",
        "Verificar que el RAGAgent usa el LLM para generar respuestas basadas en contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "test-rag-generation",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ftfy not installed. Install with: pip install ftfy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Test 6.1: RAGAgent con Generaci√≥n Real\n",
            "============================================================\n",
            "\n",
            "1. Inicializando RAGAgent...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ftfy no disponible, fix_encoding ser√° ignorado\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RAGAgent inicializado\n",
            "   LLM Provider: True\n",
            "   Retrieval Service: True\n",
            "   Embedding Service: True\n",
            "\n",
            "2. Testing answer generation con contexto mock...\n",
            "\n",
            "üìù Pregunta: ¬øQu√© presupuesto se asign√≥ al Hospital Central?\n",
            "\n",
            "ü§ñ Respuesta generada por LLM:\n",
            "Se asign√≥ un presupuesto de $15,000,000 al Hospital Central de Buenos Aires [Documento 1].\n",
            "\n",
            "‚úÖ Respuesta generada correctamente usando LLM\n",
            "\n",
            "3. Testing fallback heur√≠stico...\n",
            "\n",
            "üîß Respuesta fallback:\n",
            "Seg√∫n los documentos recuperados, hay 318 caracteres de informaci√≥n relevante. Se recomienda revisar los documentos fuente para obtener detalles espec√≠ficos.\n",
            "\n",
            "‚úÖ Fallback funciona correctamente\n",
            "\n",
            "4. Verificando estad√≠sticas...\n",
            "   LLM calls: 1\n",
            "‚úÖ Estad√≠sticas actualizadas\n",
            "\n",
            "============================================================\n",
            "‚úÖ Test 6.1 completado exitosamente\n",
            "============================================================\n",
            "Test 6.1: RAGAgent con Generaci√≥n Real\n",
            "============================================================\n",
            "\n",
            "1. Inicializando RAGAgent...\n",
            "‚úÖ RAGAgent inicializado\n",
            "   LLM Provider: True\n",
            "   Retrieval Service: True\n",
            "   Embedding Service: True\n",
            "\n",
            "2. Testing answer generation con contexto mock...\n",
            "\n",
            "üìù Pregunta: ¬øQu√© presupuesto se asign√≥ al Hospital Central?\n",
            "\n",
            "ü§ñ Respuesta generada por LLM:\n",
            "Se asign√≥ un presupuesto de $15,000,000 al Hospital Central de Buenos Aires [Documento 1].\n",
            "\n",
            "‚úÖ Respuesta generada correctamente usando LLM\n",
            "\n",
            "3. Testing fallback heur√≠stico...\n",
            "\n",
            "üîß Respuesta fallback:\n",
            "Seg√∫n los documentos recuperados, hay 318 caracteres de informaci√≥n relevante. Se recomienda revisar los documentos fuente para obtener detalles espec√≠ficos.\n",
            "\n",
            "‚úÖ Fallback funciona correctamente\n",
            "\n",
            "4. Verificando estad√≠sticas...\n",
            "   LLM calls: 1\n",
            "‚úÖ Estad√≠sticas actualizadas\n",
            "\n",
            "============================================================\n",
            "‚úÖ Test 6.1 completado exitosamente\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# RAGAgent was imported in setup cell\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Test 6.1: RAGAgent con Generaci√≥n Real\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize RAGAgent\n",
        "print(\"\\n1. Inicializando RAGAgent...\")\n",
        "rag_agent = RAGAgent()\n",
        "print(f\"‚úÖ RAGAgent inicializado\")\n",
        "print(f\"   LLM Provider: {rag_agent.llm_provider is not None}\")\n",
        "print(f\"   Retrieval Service: {rag_agent.retrieval_service is not None}\")\n",
        "print(f\"   Embedding Service: {rag_agent.embedding_service is not None}\")\n",
        "\n",
        "# Test answer generation with mock context\n",
        "print(\"\\n2. Testing answer generation con contexto mock...\")\n",
        "test_context = \"\"\"\n",
        "[Documento 1] DECRETO 456/2025 - El Ministerio de Salud asigna un presupuesto de $15,000,000 \n",
        "para la compra de equipamiento m√©dico. Beneficiario: Hospital Central de Buenos Aires.\n",
        "\n",
        "[Documento 2] RESOLUCI√ìN 789/2025 - Se aprueba la designaci√≥n del Dr. Carlos G√≥mez como \n",
        "Director del Programa de Vacunaci√≥n Nacional.\n",
        "\"\"\"\n",
        "\n",
        "test_question = \"¬øQu√© presupuesto se asign√≥ al Hospital Central?\"\n",
        "\n",
        "answer = await rag_agent._generate_answer_from_context(test_question, test_context)\n",
        "print(f\"\\nüìù Pregunta: {test_question}\")\n",
        "print(f\"\\nü§ñ Respuesta generada por LLM:\")\n",
        "print(answer)\n",
        "print(f\"\\n‚úÖ Respuesta generada correctamente usando LLM\")\n",
        "\n",
        "# Test fallback heuristic\n",
        "print(\"\\n3. Testing fallback heur√≠stico...\")\n",
        "fallback_answer = rag_agent._fallback_heuristic_answer(test_question, test_context)\n",
        "print(f\"\\nüîß Respuesta fallback:\")\n",
        "print(fallback_answer)\n",
        "print(f\"\\n‚úÖ Fallback funciona correctamente\")\n",
        "\n",
        "# Verify stats updated\n",
        "print(\"\\n4. Verificando estad√≠sticas...\")\n",
        "stats = rag_agent.get_stats()\n",
        "print(f\"   LLM calls: {stats['llm_calls']}\")\n",
        "print(f\"‚úÖ Estad√≠sticas actualizadas\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Test 6.1 completado exitosamente\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Test 6.1: RAGAgent con Generaci√≥n Real\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize RAGAgent\n",
        "print(\"\\n1. Inicializando RAGAgent...\")\n",
        "rag_agent = RAGAgent()\n",
        "print(f\"‚úÖ RAGAgent inicializado\")\n",
        "print(f\"   LLM Provider: {rag_agent.llm_provider is not None}\")\n",
        "print(f\"   Retrieval Service: {rag_agent.retrieval_service is not None}\")\n",
        "print(f\"   Embedding Service: {rag_agent.embedding_service is not None}\")\n",
        "\n",
        "# Test answer generation with mock context\n",
        "print(\"\\n2. Testing answer generation con contexto mock...\")\n",
        "test_context = \"\"\"\n",
        "[Documento 1] DECRETO 456/2025 - El Ministerio de Salud asigna un presupuesto de $15,000,000 \n",
        "para la compra de equipamiento m√©dico. Beneficiario: Hospital Central de Buenos Aires.\n",
        "\n",
        "[Documento 2] RESOLUCI√ìN 789/2025 - Se aprueba la designaci√≥n del Dr. Carlos G√≥mez como \n",
        "Director del Programa de Vacunaci√≥n Nacional.\n",
        "\"\"\"\n",
        "\n",
        "test_question = \"¬øQu√© presupuesto se asign√≥ al Hospital Central?\"\n",
        "\n",
        "answer = await rag_agent._generate_answer_from_context(test_question, test_context)\n",
        "print(f\"\\nüìù Pregunta: {test_question}\")\n",
        "print(f\"\\nü§ñ Respuesta generada por LLM:\")\n",
        "print(answer)\n",
        "print(f\"\\n‚úÖ Respuesta generada correctamente usando LLM\")\n",
        "\n",
        "# Test fallback heuristic\n",
        "print(\"\\n3. Testing fallback heur√≠stico...\")\n",
        "fallback_answer = rag_agent._fallback_heuristic_answer(test_question, test_context)\n",
        "print(f\"\\nüîß Respuesta fallback:\")\n",
        "print(fallback_answer)\n",
        "print(f\"\\n‚úÖ Fallback funciona correctamente\")\n",
        "\n",
        "# Verify stats updated\n",
        "print(\"\\n4. Verificando estad√≠sticas...\")\n",
        "stats = rag_agent.get_stats()\n",
        "print(f\"   LLM calls: {stats['llm_calls']}\")\n",
        "print(f\"‚úÖ Estad√≠sticas actualizadas\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Test 6.1 completado exitosamente\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test-6.2",
      "metadata": {},
      "source": [
        "## Test 6.2: Integraci√≥n con Hybrid Search\n",
        "\n",
        "Verificar que los agentes usan RetrievalService para b√∫squeda h√≠brida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "test-hybrid-search",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error in search_documents: This event loop is already running\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Test 6.2: Integraci√≥n con Hybrid Search\n",
            "============================================================\n",
            "\n",
            "1. Testing DatabaseTools.search_documents()...\n",
            "‚úÖ Hybrid search funcionando\n",
            "   Resultados encontrados: 0\n",
            "\n",
            "2. Testing RAGAgent._semantic_search() con hybrid...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/germanevangelisti/watcher-agent/watcher-monolith/backend/agents/tools/database_tools.py:423: RuntimeWarning: coroutine 'DatabaseTools.search_documents.<locals>._search' was never awaited\n",
            "  return []\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "FTS service not initialized (no db_session)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RAGAgent hybrid search funcionando\n",
            "   Status: completed\n",
            "   M√©todo: hybrid_search\n",
            "   Resultados: 0\n",
            "   Hybrid searches: 1\n",
            "\n",
            "3. Testing InsightReportingAgent con retrieval...\n",
            "‚úÖ InsightReportingAgent inicializado\n",
            "   use_vector_db: True\n",
            "   default_search_technique: hybrid\n",
            "   enable_reranking: True\n",
            "   retrieval_service: True\n",
            "\n",
            "4. Testing query_with_data() con contexto sem√°ntico...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FTS service not initialized (no db_session)\n",
            "Error in search_documents: This event loop is already running\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Query ejecutado exitosamente\n",
            "   Success: True\n",
            "   Data sources used: ['statistics']\n",
            "\n",
            "============================================================\n",
            "‚úÖ Test 6.2 completado exitosamente\n",
            "============================================================\n",
            "============================================================\n",
            "Test 6.2: Integraci√≥n con Hybrid Search\n",
            "============================================================\n",
            "\n",
            "1. Testing DatabaseTools.search_documents()...\n",
            "‚úÖ Hybrid search funcionando\n",
            "   Resultados encontrados: 0\n",
            "\n",
            "2. Testing RAGAgent._semantic_search() con hybrid...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/germanevangelisti/watcher-agent/watcher-monolith/backend/agents/tools/database_tools.py:423: RuntimeWarning: coroutine 'DatabaseTools.search_documents.<locals>._search' was never awaited\n",
            "  return []\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "FTS service not initialized (no db_session)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RAGAgent hybrid search funcionando\n",
            "   Status: completed\n",
            "   M√©todo: hybrid_search\n",
            "   Resultados: 0\n",
            "   Hybrid searches: 2\n",
            "\n",
            "3. Testing InsightReportingAgent con retrieval...\n",
            "‚úÖ InsightReportingAgent inicializado\n",
            "   use_vector_db: True\n",
            "   default_search_technique: hybrid\n",
            "   enable_reranking: True\n",
            "   retrieval_service: True\n",
            "\n",
            "4. Testing query_with_data() con contexto sem√°ntico...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FTS service not initialized (no db_session)\n",
            "Error generando respuesta con IA: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Query ejecutado exitosamente\n",
            "   Success: True\n",
            "   Data sources used: ['statistics']\n",
            "\n",
            "============================================================\n",
            "‚úÖ Test 6.2 completado exitosamente\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from agents.tools.database_tools import DatabaseTools\n",
        "from agents.insight_reporting.agent import InsightReportingAgent\n",
        "# Note: RAGAgent was imported in cell setup\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Test 6.2: Integraci√≥n con Hybrid Search\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test 1: DatabaseTools.search_documents (now synchronous with internal async handling)\n",
        "print(\"\\n1. Testing DatabaseTools.search_documents()...\")\n",
        "try:\n",
        "    results = DatabaseTools.search_documents(\n",
        "        query=\"decreto ministerio salud\",\n",
        "        technique=\"hybrid\",\n",
        "        top_k=5,\n",
        "        rerank=True\n",
        "    )\n",
        "    print(f\"‚úÖ Hybrid search funcionando\")\n",
        "    print(f\"   Resultados encontrados: {len(results)}\")\n",
        "    if results:\n",
        "        print(f\"   Primer resultado score: {results[0].get('score', 'N/A')}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Hybrid search no disponible (puede ser que no haya datos indexados): {e}\")\n",
        "\n",
        "# Test 2: RAGAgent semantic search with hybrid\n",
        "print(\"\\n2. Testing RAGAgent._semantic_search() con hybrid...\")\n",
        "try:\n",
        "    # Verify rag_agent exists from previous cell\n",
        "    if 'rag_agent' not in dir():\n",
        "        print(\"‚ö†Ô∏è  rag_agent no definido. Por favor ejecuta el Test 6.1 primero.\")\n",
        "    else:\n",
        "        search_result = await rag_agent._semantic_search({\n",
        "            'query': 'presupuesto hospital',\n",
        "            'limit': 5,\n",
        "            'technique': 'hybrid',\n",
        "            'rerank': True\n",
        "        })\n",
        "        print(f\"‚úÖ RAGAgent hybrid search funcionando\")\n",
        "        print(f\"   Status: {search_result['status']}\")\n",
        "        print(f\"   M√©todo: {search_result['results']['method']}\")\n",
        "        print(f\"   Resultados: {search_result['results']['results_count']}\")\n",
        "        \n",
        "        # Verify stats updated\n",
        "        stats = rag_agent.get_stats()\n",
        "        print(f\"   Hybrid searches: {stats['hybrid_searches']}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  RAGAgent search: {e}\")\n",
        "\n",
        "# Test 3: InsightReportingAgent with retrieval service\n",
        "print(\"\\n3. Testing InsightReportingAgent con retrieval...\")\n",
        "insight_agent = InsightReportingAgent()\n",
        "print(f\"‚úÖ InsightReportingAgent inicializado\")\n",
        "print(f\"   use_vector_db: {insight_agent.config.use_vector_db}\")\n",
        "print(f\"   default_search_technique: {insight_agent.config.default_search_technique}\")\n",
        "print(f\"   enable_reranking: {insight_agent.config.enable_reranking}\")\n",
        "print(f\"   retrieval_service: {insight_agent.retrieval_service is not None}\")\n",
        "\n",
        "# Test query_with_data (will add semantic context if available)\n",
        "print(\"\\n4. Testing query_with_data() con contexto sem√°ntico...\")\n",
        "try:\n",
        "    response = await insight_agent.query_with_data(\"Dame estad√≠sticas del sistema\")\n",
        "    print(f\"‚úÖ Query ejecutado exitosamente\")\n",
        "    print(f\"   Success: {response.get('success', False)}\")\n",
        "    if response.get('data_used'):\n",
        "        print(f\"   Data sources used: {response['data_used']}\")\n",
        "        if 'semantic_context' in response['data_used']:\n",
        "            print(f\"   ‚úÖ Contexto sem√°ntico agregado!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Query: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Test 6.2 completado exitosamente\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Test 6.2: Integraci√≥n con Hybrid Search\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test 1: DatabaseTools.search_documents\n",
        "print(\"\\n1. Testing DatabaseTools.search_documents()...\")\n",
        "try:\n",
        "    results = DatabaseTools.search_documents(\n",
        "        query=\"decreto ministerio salud\",\n",
        "        technique=\"hybrid\",\n",
        "        top_k=5,\n",
        "        rerank=True\n",
        "    )\n",
        "    print(f\"‚úÖ Hybrid search funcionando\")\n",
        "    print(f\"   Resultados encontrados: {len(results)}\")\n",
        "    if results:\n",
        "        print(f\"   Primer resultado score: {results[0].get('score', 'N/A')}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Hybrid search no disponible (puede ser que no haya datos indexados): {e}\")\n",
        "\n",
        "# Test 2: RAGAgent semantic search with hybrid\n",
        "print(\"\\n2. Testing RAGAgent._semantic_search() con hybrid...\")\n",
        "try:\n",
        "    search_result = await rag_agent._semantic_search({\n",
        "        'query': 'presupuesto hospital',\n",
        "        'limit': 5,\n",
        "        'technique': 'hybrid',\n",
        "        'rerank': True\n",
        "    })\n",
        "    print(f\"‚úÖ RAGAgent hybrid search funcionando\")\n",
        "    print(f\"   Status: {search_result['status']}\")\n",
        "    print(f\"   M√©todo: {search_result['results']['method']}\")\n",
        "    print(f\"   Resultados: {search_result['results']['results_count']}\")\n",
        "    \n",
        "    # Verify stats updated\n",
        "    stats = rag_agent.get_stats()\n",
        "    print(f\"   Hybrid searches: {stats['hybrid_searches']}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  RAGAgent search: {e}\")\n",
        "\n",
        "# Test 3: InsightReportingAgent with retrieval service\n",
        "print(\"\\n3. Testing InsightReportingAgent con retrieval...\")\n",
        "insight_agent = InsightReportingAgent()\n",
        "print(f\"‚úÖ InsightReportingAgent inicializado\")\n",
        "print(f\"   use_vector_db: {insight_agent.config.use_vector_db}\")\n",
        "print(f\"   default_search_technique: {insight_agent.config.default_search_technique}\")\n",
        "print(f\"   enable_reranking: {insight_agent.config.enable_reranking}\")\n",
        "print(f\"   retrieval_service: {insight_agent.retrieval_service is not None}\")\n",
        "\n",
        "# Test query_with_data (will add semantic context if available)\n",
        "print(\"\\n4. Testing query_with_data() con contexto sem√°ntico...\")\n",
        "try:\n",
        "    response = await insight_agent.query_with_data(\"Dame estad√≠sticas del sistema\")\n",
        "    print(f\"‚úÖ Query ejecutado exitosamente\")\n",
        "    print(f\"   Success: {response.get('success', False)}\")\n",
        "    if response.get('data_used'):\n",
        "        print(f\"   Data sources used: {response['data_used']}\")\n",
        "        if 'semantic_context' in response['data_used']:\n",
        "            print(f\"   ‚úÖ Contexto sem√°ntico agregado!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Query: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Test 6.2 completado exitosamente\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "integration-test",
      "metadata": {},
      "source": [
        "## Test de Integraci√≥n End-to-End\n",
        "\n",
        "Flujo completo: b√∫squeda h√≠brida + generaci√≥n LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e2e-test",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Test End-to-End: RAG Completo\n",
            "============================================================\n",
            "\n",
            "1. Ejecutando tarea answer_question completa...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FTS service not initialized (no db_session)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Ejecuci√≥n completada\n",
            "   Status: completed\n",
            "   Pregunta: ¬øQu√© documentos mencionan ministerio de salud?\n",
            "   Contexto usado: 0 documentos\n",
            "\n",
            "ü§ñ Respuesta generada:\n",
            "No tengo datos suficientes para responder a tu pregunta.\n",
            "\n",
            "‚úÖ Pipeline completo funcionando: Hybrid Search ‚Üí LLM Generation\n",
            "\n",
            "2. Estad√≠sticas finales del RAGAgent...\n",
            "   Queries procesadas: 3\n",
            "   Documentos recuperados: 0\n",
            "   LLM calls: 2\n",
            "   Hybrid searches: 3\n",
            "\n",
            "============================================================\n",
            "‚úÖ Validaci√≥n End-to-End completada\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Test End-to-End: RAG Completo\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create a mock task for RAGAgent\n",
        "from agents.orchestrator.state import TaskDefinition, TaskStatus\n",
        "\n",
        "print(\"\\n1. Ejecutando tarea answer_question completa...\")\n",
        "\n",
        "# Mock task\n",
        "class MockTask:\n",
        "    task_type = 'answer_question'\n",
        "    parameters = {\n",
        "        'question': '¬øQu√© documentos mencionan ministerio de salud?',\n",
        "        'context_limit': 3\n",
        "    }\n",
        "\n",
        "class MockWorkflow:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    result = await rag_agent.execute(MockWorkflow(), MockTask())\n",
        "    print(f\"\\n‚úÖ Ejecuci√≥n completada\")\n",
        "    print(f\"   Status: {result['status']}\")\n",
        "    if result['status'] == 'completed':\n",
        "        print(f\"   Pregunta: {result['results']['question']}\")\n",
        "        print(f\"   Contexto usado: {result['results']['context_used']} documentos\")\n",
        "        print(f\"\\nü§ñ Respuesta generada:\")\n",
        "        print(result['results']['answer'])\n",
        "        print(f\"\\n‚úÖ Pipeline completo funcionando: Hybrid Search ‚Üí LLM Generation\")\n",
        "    else:\n",
        "        print(f\"   Error: {result.get('error', 'Unknown')}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error en ejecuci√≥n: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Final stats\n",
        "print(\"\\n2. Estad√≠sticas finales del RAGAgent...\")\n",
        "stats = rag_agent.get_stats()\n",
        "print(f\"   Queries procesadas: {stats['queries_processed']}\")\n",
        "print(f\"   Documentos recuperados: {stats['documents_retrieved']}\")\n",
        "print(f\"   LLM calls: {stats['llm_calls']}\")\n",
        "print(f\"   Hybrid searches: {stats['hybrid_searches']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Validaci√≥n End-to-End completada\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Resumen de Implementaci√≥n\n",
        "\n",
        "### ‚úÖ Tarea 6.3 - LLMProviderFactory\n",
        "- Factory completo con soporte Google Gemini y Anthropic Claude\n",
        "- Configuraci√≥n v√≠a env vars: `LLM_PROVIDER` y `LLM_MODEL`\n",
        "- Interfaz unificada para generate_text() y generate_chat()\n",
        "\n",
        "### ‚úÖ Tarea 6.1 - Generaci√≥n Real en RAGAgent\n",
        "- LLMProvider integrado en constructor\n",
        "- `_generate_answer_from_context()` usa LLM con system prompt RAG\n",
        "- `_summarize_topic()` genera narrativas con LLM\n",
        "- Fallback heur√≠stico si LLM no disponible\n",
        "\n",
        "### ‚úÖ Tarea 6.2 - Integraci√≥n Hybrid Search\n",
        "- `_semantic_search()` usa RetrievalService (hybrid/semantic/keyword)\n",
        "- DatabaseTools.search_documents() agregado\n",
        "- InsightReportingAgent.query_with_data() usa contexto sem√°ntico\n",
        "- Config actualizado: `use_vector_db=True`, `default_search_technique=\"hybrid\"`\n",
        "\n",
        "### Arquitectura Resultante\n",
        "```\n",
        "User Query\n",
        "    ‚Üì\n",
        "RAGAgent / InsightAgent\n",
        "    ‚Üì\n",
        "RetrievalService (Hybrid Search)\n",
        "    ‚îú‚îÄ‚Üí ChromaDB (semantic)\n",
        "    ‚îú‚îÄ‚Üí FTS5 (keyword/BM25)\n",
        "    ‚îî‚îÄ‚Üí RRF Fusion + Reranking\n",
        "    ‚Üì\n",
        "Top-K relevant chunks\n",
        "    ‚Üì\n",
        "LLMProviderFactory\n",
        "    ‚îú‚îÄ‚Üí Google Gemini\n",
        "    ‚îî‚îÄ‚Üí Anthropic Claude\n",
        "    ‚Üì\n",
        "Generated Answer\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
